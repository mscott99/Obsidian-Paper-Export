\documentclass{article}
\input{header}
\input{preamble.sty}
\addbibresource{bibliography.bib}

\title{Hello World}
\author{Matthew}

\begin{document}
\maketitle
\begin{abstract}
    Hello blah bldsjlfdsjk sjdklfs


\end{abstract}

\section{Introduction}\label{section:Introduction}
More text

\section{Main Results}\label{section:Main Results}
We start with a definition:

\begin{definition}
    \label{(k,d,n)-Generative Network with Piecewise Affine Activations}
    Define a (k,d,n)-generative net with layer widths  $k_{0}, k_1, \dots, k_{d}$  with  $k = k_0$  and  $k_d = n$  and coefficients  $\{ l_i \}_{i \in [d]}\subseteq \mathbb{N}^d$  to be a function  $G:\field^k \to \field^n$  of the form



    \begin{align*}
        G(z) = \sigma_d \left( W^{(d)} \sigma_{d-1}\left( \cdots W^{(2)} \sigma_1 \left( W^{(1)} z \right) \right)\right),
    \end{align*}

    where for each  $i \in [d]$ ,   $\sigma_i:\mathbb{R}^{k_i}\to\mathbb{R}^{k_i}$  is a piecewise affine function with pieces generated by no more than  $l_i k_i$  hyperplanes.


\end{definition}

Our main theorem is the following:

\begin{theorem}
    \label{Self-Difference of Affine Neural Net is Contained in Conical Piecewise Linear Function with Counted Pieces}
    The self-difference of the range of a (k,d,n)-Generative Network\ref{(k,d,n)-Generative Network with Piecewise Affine Activations}  $G$  is contained in the range of a positive-homogeneous piecewise linear function  $\tilde{G}:\field^{2k+1}\to \field^{n}$  with a number of linear conical pieces  $M$  such that


    \begin{equation*}
        \log M \leq 2kd\log\left( \frac{e s}{k} \right),
    \end{equation*}

    where


    \begin{equation*}
        s = \left( \prod_{i=1}^d \max(k, k_i)l_i \right)^{1/d}.
    \end{equation*}


\end{theorem}


\section{Proofs}\label{section:Proofs}
We now prove the main result:

\begin{proof}
    Notice that  $G$  is a piecewise affine function with  $N$  pieces and

    \autoref{Counting the Number of Affine Components of a Neural Net with Affine Activations}


    \begin{lemma}
        \label{Counting the Number of Affine Components of a Neural Net with Affine Activations}
        A (k,d,n)-Generative Network\ref{(k,d,n)-Generative Network with Piecewise Affine Activations} will have at most


        \begin{equation*}
            \log N \leq kd \log\left(\frac{e s}{k}\right)
        \end{equation*}

        affine pieces where


        \begin{equation*}
            s := \left( \prod_{i=1}^d \max(k, k_i)l_i \right)^{1/d}
        \end{equation*}

        is a geometric mean of the weighted widths of the layers of the network.


    \end{lemma}


    It follows that  $(x,y) \to G(x) - G(y)$  is also piecewise affine and has no more than  $2N$  pieces.

    Then from

    \autoref{Piecewise Affine Function is Contained in Conical Piecewise Linear Function with Extended Domain}


    \begin{lemma}
        \label{Piecewise Affine Function is Contained in Conical Piecewise Linear Function with Extended Domain}
        Let  $f:\mathbb{R}^k\to\mathbb{R}^n$  be piecewise affine with  $N$  pieces. Then there exists a function  $\tilde{f}:\mathbb{R}^{k+1} \to \mathbb{R}^n$  which is piecewise linear with conical pieces such that  $\range(\tilde{f})\supseteq\range(f)$ . Furthermore,  $\tilde{f}((x,1)) \equiv f(x)$  and in the case that  $f$  has polyhedral pieces,  $\tilde{f}$  will have pieces that are polyhedral cones.


    \end{lemma}


    there exists a function  $\tilde{G}:\mathbb{R}^{2k + 1} \to \mathbb{R}^n$ , and one can check that it has all the properties in the statement.


\end{proof}

We used two lemmas in the previous proof; \autoref{Counting the Number of Affine Components of a Neural Net with Affine Activations}, \autoref{Piecewise Affine Function is Contained in Conical Piecewise Linear Function with Extended Domain}. These are based on \cite{vershyninHighDimensionalProbabilityIntroduction2018}.
We now show \autoref{Counting the Number of Affine Components of a Neural Net with Affine Activations}.

\begin{proof}
    From the definition of a \autoref{(k,d,n)-Generative Network with Piecewise Affine Activations},  the  $ith$  layer has an activation that is piecewise affine with no more than  $l_i k_i$  affine pieces. This  $ith$  layer will have a in its domain contained in a finite number of subspaces that are of dimension of no more than  $\min(k,k_i)$ . Therefore consider the  $ith$  activation acting on a  $\min(k,k_i)-$ dimensional subspace. By

    \autoref{Counting the Number of Cell Crossings of a Subspace}


    \begin{lemma}
        \label{Counting the Number of Cell Crossings of a Subspace}
        Let  $U$  be a  $k-$ dimensional hyperplane in general position in  $\field^n$ . Let there be  $P$  hyperplanes  $\{ H_i \}_{i \in [P]}$  in  $\field^n$ . These hyperplanes induce connected components between the hyperplanes that we call "cells". The subspace  $U$  may intersect at most


        \begin{equation*}
            \sum_{i=0}^k {P \choose i} \leq \left( \frac{eP}{k} \right)^k   \\
        \end{equation*}

        cells.


    \end{lemma}


    , the layer will generate no more than  $\left( \frac{el_i k_i}{\min(k, k_i)} \right)^{\min(k,k_i)}$   pieces for each subspace in its domain. Notice that this expression is monotonically increasing in  $k_i$ . Therefore, by increase  $k_i$  to  $\max(k,k_i)$  we get the upper bound


    \begin{equation*}
        \left(  \frac{el_i \max(k_i, k)}{k} \right)^{k}.
    \end{equation*}

    It follows that the number of affine pieces  $N$  of the full network is such that


    \begin{equation*}
        N \leq \prod_{i=1}^d \left( \frac{el_i\max(k,k_i)}{k} \right)^k \leq \left(\frac{es}{k}\right)^{kd},
    \end{equation*}

    where


    \begin{equation*}
        s := \left( \prod_{i=1}^d \max(k, k_i)l_i \right)^{1/d}.
    \end{equation*}


\end{proof}

And we now give the proof of lemma \autoref{Piecewise Affine Function is Contained in Conical Piecewise Linear Function with Extended Domain};

\begin{proof}
    Letting  $S$  be the finite partition of  $dom(f)$  corresponding to the affine pieces of  $f$ , we have that  $f(x) = \sum_{s \in S} W_s x + b_s$  for some  $R^{n \times k}$  matrices  $\{ W_s \}$  and some vectors  $\{ b_s \}\subseteq \mathbb{R}^k$ . Let  $s_0 \in S$ . For any vector  $\tilde{x} \in \mathbb{R}^{k+1}$  with  $\tilde{x} = (\tilde{x}_{[k]}, \tilde{x}_{k+1})$ , let


    \begin{equation*}
        \tilde{f}(\tilde{x}) := \indicator_{\tilde{x}_k \leq 0} (W_{s_0} \tilde{x}_[k]+ b_{s_0}\tilde{x}_k) + \indicator_{\tilde{x}_k > 0} \sum_{s \in S} \indicator_{\frac{1}{\tilde{x}_k}\tilde{x}_{[k]} \in s} (W_s \tilde{x}_[k] + b_s \tilde{x}_k).
    \end{equation*}

    One can check that for this construction all the properties described in the statement hold.


\end{proof}

Created: 2023-03-27\ref{Temporal/Daily Notes/2023-03-27}

\printbibliography
\end{document}
