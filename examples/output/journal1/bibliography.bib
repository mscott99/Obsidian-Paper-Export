@online{ActivationFunctionDesign,
  title = {Activation Function Design for Deep Networks: Linearity and Effective Initialisation | {{Elsevier Enhanced Reader}}},
  shorttitle = {Activation Function Design for Deep Networks},
  doi = {10.1016/j.acha.2021.12.010},
  url = {https://reader.elsevier.com/reader/sd/pii/S1063520321001111?token=6602EF9BFD0FF36E66DA2272070F885122578D1292614BDD28CAE4E01666029CB2BC320C487DA9356448D89EDEBF435D&originRegion=us-east-1&originCreation=20221018172411},
  urldate = {2022-10-18},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/K84RXRBT/Activation function design for deep networks line.pdf;/Users/matthewscott/Zotero/storage/NFQP9WNJ/S1063520321001111.html}
}

@book{adcockCompressiveImagingStructure2021,
  title = {Compressive {{Imaging}}: {{Structure}}, {{Sampling}}, {{Learning}}},
  shorttitle = {Compressive {{Imaging}}},
  author = {Adcock, Ben and Hansen, Anders C.},
  date = {2021-09-16},
  edition = {1st edition},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge, United Kingdom New York, NY, USA Port Melbourne, Australia New Delhi, India Singapore}},
  isbn = {978-1-108-42161-4},
  langid = {english},
  pagetotal = {614},
  file = {/Users/matthewscott/Zotero/storage/5YBLUADU/Adcock and Hansen - 2021 - Compressive Imaging Structure, Sampling, Learning.pdf}
}

@article{adcockOracletypeLocalRecovery2021,
  title = {On Oracle-Type Local Recovery Guarantees in Compressed Sensing},
  author = {Adcock, Ben and Boyer, Claire and Brugiapaglia, Simone},
  date = {2021-03-16},
  journaltitle = {Information and Inference: A Journal of the IMA},
  volume = {10},
  number = {1},
  eprint = {1806.03789},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  pages = {1--49},
  issn = {2049-8764, 2049-8772},
  doi = {10.1093/imaiai/iaaa007},
  url = {http://arxiv.org/abs/1806.03789},
  urldate = {2022-12-15},
  abstract = {We present improved sampling complexity bounds for stable and robust sparse recovery in compressed sensing. Our unified analysis based on l1 minimization encompasses the case where (i) the measurements are block-structured samples in order to reflect the structured acquisition that is often encountered in applications; (ii) the signal has an arbitrary structured sparsity, by results depending on its support S. Within this framework and under a random sign assumption, the number of measurements needed by l1 minimization can be shown to be of the same order than the one required by an oracle least-squares estimator. Moreover, these bounds can be minimized by adapting the variable density sampling to a given prior on the signal support and to the coherence of the measurements. We illustrate both numerically and analytically that our results can be successfully applied to recover Haar wavelet coefficients that are sparse in levels from random Fourier measurements in dimension one and two, which can be of particular interest in imaging problems. Finally, a preliminary numerical investigation shows the potential of this theory for devising adaptive sampling strategies in sparse polynomial approximation.},
  keywords = {Computer Science - Information Theory},
  file = {/Users/matthewscott/Zotero/storage/6NZET7G4/Adcock et al. - 2021 - On oracle-type local recovery guarantees in compre.pdf;/Users/matthewscott/Zotero/storage/UXGH5HJA/1806.html}
}

@book{adcockSparsePolynomialApproximation2022,
  title = {Sparse {{Polynomial Approximation}} of {{High-Dimensional Functions}}},
  author = {Adcock, Ben and Brugiapaglia, Simone and Webster, Clayton G.},
  date = {2022-02-15},
  publisher = {{SIAM - Society for Industrial and Applied Mathematics}},
  location = {{Philadelphia}},
  isbn = {978-1-61197-687-8},
  langid = {english},
  pagetotal = {292},
  file = {/Users/matthewscott/Zotero/storage/6RMNYXJR/Adcock et al. - 2022 - Sparse Polynomial Approximation of High-Dimensiona.pdf}
}

@book{ahrensHowTakeSmart2017,
  title = {How to {{Take Smart Notes}}: {{One Simple Technique}} to {{Boost Writing}}, {{Learning}} and {{Thinking}} – for {{Students}}, {{Academics}} and {{Nonfiction Book Writers}}},
  shorttitle = {How to {{Take Smart Notes}}},
  author = {Ahrens, Sönke},
  date = {2017-02-21},
  eprint = {8drrAQAACAAJ},
  eprinttype = {googlebooks},
  publisher = {{Sönke Ahrens}},
  abstract = {The key to good and efficient writing lies in the intelligent organisation of ideas and notes. This book helps students, academics and nonfiction writers to get more done, write intelligent texts and learn for the long run. It teaches you how to take smart notes and ensure they bring you and your projects forward. The Take Smart Notes principle is based on established psychological insight and draws from a tried and tested note-taking-technique. This is the first comprehensive guide and description of this system in English, and not only does it explain how it works, but also why. It suits students and academics in the social sciences and humanities, nonfiction writers and others who are in the business of reading, thinking and writing. Instead of wasting your time searching for notes, quotes or references, you can focus on what really counts: thinking, understanding and developing new ideas in writing. It does not matter if you prefer taking notes with pen and paper or on a computer, be it Windows, Mac or Linux. And you can start right away.},
  langid = {english},
  pagetotal = {188},
  keywords = {Reference / Writing Skills,Self-Help / Personal Growth / Memory Improvement,Self-Help / Self-Management / Time Management,Study Aids / Book Notes}
}

@book{allenGettingThingsDone2015,
  title = {Getting {{Things Done}}: {{The Art}} of {{Stress-Free Productivity}}},
  shorttitle = {Getting {{Things Done}}},
  author = {Allen, David},
  date = {2015-03-17},
  eprint = {7PoYBAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Penguin}},
  abstract = {The book Lifehack calls~"The Bible of business and personal productivity.""A completely revised and updated edition of the blockbuster bestseller from 'the personal productivity guru'"—Fast CompanySince it was first published almost fifteen years ago, David Allen’s Getting Things Done has become one of the most influential business books of its era, and the ultimate book on personal organization. “GTD” is now shorthand for an entire way of approaching professional and personal tasks, and has spawned an entire culture of websites, organizational tools, seminars, and offshoots.~Allen has rewritten the book from start to finish, tweaking his classic text with important perspectives on the new workplace, and adding material that will make the book fresh and relevant for years to come. This new edition of Getting Things Done will be welcomed not only by its hundreds of thousands of existing fans but also by a whole new generation eager to adopt its proven principles.},
  isbn = {978-0-698-16186-3},
  langid = {english},
  pagetotal = {354},
  keywords = {Business & Economics / Time Management,Self-Help / Self-Management / Stress Management,Self-Help / Self-Management / Time Management}
}

@article{aroraLectureGuestLecture,
  title = {Lecture 0: {{Guest Lecture}} by {{Venkat Guruswami}}: {{Euclidean Subspaces}} and {{Compressed Sensing}}},
  author = {Arora, Sanjeev and Hardt, Moritz},
  pages = {5},
  langid = {english},
  keywords = {Almost Euclidean},
  file = {/Users/matthewscott/Zotero/storage/NSHLYKKI/Arora and Hardt - Lecture 0 Guest Lecture by Venkat Guruswami Eucl.pdf}
}

@online{bakhshizadehSharpConcentrationResults2022,
  title = {Sharp {{Concentration Results}} for {{Heavy-Tailed Distributions}}},
  author = {Bakhshizadeh, Milad and Maleki, Arian and family=Pena, given=Victor H., prefix=de la, useprefix=true},
  date = {2022-07-25},
  number = {arXiv:2003.13819},
  eprint = {arXiv:2003.13819},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2003.13819},
  url = {http://arxiv.org/abs/2003.13819},
  urldate = {2023-02-23},
  abstract = {We obtain concentration and large deviation for the sums of independent and identically distributed random variables with heavy-tailed distributions. Our concentration results are concerned with random variables whose distributions satisfy \$\textbackslash mathbb\{P\}(X{$>$}t) \textbackslash leq \{\textbackslash rm e\}\^\{- I(t)\}\$, where \$I: \textbackslash mathbb\{R\} \textbackslash rightarrow \textbackslash mathbb\{R\}\$ is an increasing function and \$I(t)/t \textbackslash rightarrow \textbackslash alpha \textbackslash in [0, \textbackslash infty)\$ as \$t \textbackslash rightarrow \textbackslash infty\$. Our main theorem can not only recover some of the existing results, such as the concentration of the sum of subWeibull random variables, but it can also produce new results for the sum of random variables with heavier tails. We show that the concentration inequalities we obtain are sharp enough to offer large deviation results for the sums of independent random variables as well. Our analyses which are based on standard truncation arguments simplify, unify and generalize the existing results on the concentration and large deviation of heavy-tailed random variables.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Mathematics - Probability,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/matthewscott/Zotero/storage/7KNHLL63/Bakhshizadeh et al. - 2022 - Sharp Concentration Results for Heavy-Tailed Distr.pdf;/Users/matthewscott/Zotero/storage/RVCRYFYA/2003.html}
}

@online{baLayerNormalization2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  date = {2016-07-21},
  number = {arXiv:1607.06450},
  eprint = {arXiv:1607.06450},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1607.06450},
  url = {http://arxiv.org/abs/1607.06450},
  urldate = {2022-11-27},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,machine learning,Statistics - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/KTIM232H/Ba et al. - 2016 - Layer Normalization.pdf;/Users/matthewscott/Zotero/storage/CCGGIC7S/1607.html}
}

@article{baldassarreLearningbasedCompressiveSubsampling2016,
  title = {Learning-Based {{Compressive Subsampling}}},
  author = {Baldassarre, Luca and Li, Yen-Huan and Scarlett, Jonathan and Gözcü, Baran and Bogunovic, Ilija and Cevher, Volkan},
  date = {2016-06},
  journaltitle = {IEEE Journal of Selected Topics in Signal Processing},
  shortjournal = {IEEE J. Sel. Top. Signal Process.},
  volume = {10},
  number = {4},
  eprint = {1510.06188},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  pages = {809--822},
  issn = {1932-4553, 1941-0484},
  doi = {10.1109/JSTSP.2016.2548442},
  url = {http://arxiv.org/abs/1510.06188},
  urldate = {2022-12-15},
  abstract = {The problem of recovering a structured signal \$\textbackslash mathbf\{x\} \textbackslash in \textbackslash mathbb\{C\}\^p\$ from a set of dimensionality-reduced linear measurements \$\textbackslash mathbf\{b\} = \textbackslash mathbf \{A\}\textbackslash mathbf \{x\}\$ arises in a variety of applications, such as medical imaging, spectroscopy, Fourier optics, and computerized tomography. Due to computational and storage complexity or physical constraints imposed by the problem, the measurement matrix \$\textbackslash mathbf\{A\} \textbackslash in \textbackslash mathbb\{C\}\^\{n \textbackslash times p\}\$ is often of the form \$\textbackslash mathbf\{A\} = \textbackslash mathbf\{P\}\_\{\textbackslash Omega\}\textbackslash boldsymbol\{\textbackslash Psi\}\$ for some orthonormal basis matrix \$\textbackslash boldsymbol\{\textbackslash Psi\}\textbackslash in \textbackslash mathbb\{C\}\^\{p \textbackslash times p\}\$ and subsampling operator \$\textbackslash mathbf\{P\}\_\{\textbackslash Omega\}: \textbackslash mathbb\{C\}\^\{p\} \textbackslash rightarrow \textbackslash mathbb\{C\}\^\{n\}\$ that selects the rows indexed by \$\textbackslash Omega\$. This raises the fundamental question of how best to choose the index set \$\textbackslash Omega\$ in order to optimize the recovery performance. Previous approaches to addressing this question rely on non-uniform \textbackslash emph\{random\} subsampling using application-specific knowledge of the structure of \$\textbackslash mathbf\{x\}\$. In this paper, we instead take a principled learning-based approach in which a \textbackslash emph\{fixed\} index set is chosen based on a set of training signals \$\textbackslash mathbf\{x\}\_1,\textbackslash dotsc,\textbackslash mathbf\{x\}\_m\$. We formulate combinatorial optimization problems seeking to maximize the energy captured in these signals in an average-case or worst-case sense, and we show that these can be efficiently solved either exactly or approximately via the identification of modularity and submodularity structures. We provide both deterministic and statistical theoretical guarantees showing how the resulting measurement matrices perform on signals differing from the training signals, and we provide numerical examples showing our approach to be effective on a variety of data sets.},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/VQYJZSIJ/Baldassarre et al. - 2016 - Learning-based Compressive Subsampling.pdf;/Users/matthewscott/Zotero/storage/P4M73HH9/1510.html}
}

@article{baldiNeuronalCapacity2019,
  title = {On Neuronal Capacity},
  author = {Baldi, Pierre and Vershynin, Roman},
  date = {2019-12-20},
  journaltitle = {Journal of Statistical Mechanics: Theory and Experiment},
  shortjournal = {J. Stat. Mech.},
  volume = {2019},
  number = {12},
  pages = {124012},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/ab3285},
  url = {https://iopscience.iop.org/article/10.1088/1742-5468/ab3285},
  urldate = {2022-08-13},
  abstract = {We define the capacity of a learning machine to be the logarithm of the number (or volume) of the functions it can implement. We review known results, and derive new results, estimating the capacity of several neuronal models: linear and polynomial threshold gates, linear and polynomial threshold gates with constrained weights (binary weights, positive weights), and ReLU neurons. We also derive some capacity estimates and bounds for fully recurrent networks, as well as feedforward networks.},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/TK6VWEGD/Baldi and Vershynin - 2019 - On neuronal capacity.pdf}
}

@article{baraniukRandomProjectionsSmooth2009,
  title = {Random {{Projections}} of {{Smooth Manifolds}}},
  author = {Baraniuk, Richard G. and Wakin, Michael B.},
  date = {2009-02},
  journaltitle = {Foundations of Computational Mathematics},
  shortjournal = {Found Comput Math},
  volume = {9},
  number = {1},
  pages = {51--77},
  issn = {1615-3375, 1615-3383},
  doi = {10.1007/s10208-007-9011-z},
  url = {http://link.springer.com/10.1007/s10208-007-9011-z},
  urldate = {2022-08-12},
  abstract = {Many types of data and information can be described by concise models that suggest each data vector (or signal) actually has “few degrees of freedom” relative to its size N . This is the motivation for a variety of dimensionality reduction techniques for data processing that attempt to reduce or eliminate the impact of the ambient dimension N on computational or storage requirements. As an example, many signals can be expressed as a sparse linear combination of elements from some dictionary. The sparsity of the representation directly reflects the conciseness of the model and permits efficient techniques such as Compressed Sensing (CS), an emerging theory for sparse signal recovery requiring only a small number of nonadaptive, random linear measurements.},
  langid = {english},
  keywords = {53A07,57R40,62H99,65C99,68P30,68T05,94A12,94A29,Compressed sensing,Dimensionality reduction,Johnson–Lindenstrauss lemma,Manifold learning,Manifolds,Random projections,Recovery,Sparsity},
  file = {/Users/matthewscott/Zotero/storage/CKGJSIPX/Baraniuk and Wakin - 2009 - Random Projections of Smooth Manifolds.pdf}
}

@article{berkCoherenceParameterCharacterizing2022,
  title = {A Coherence Parameter Characterizing Generative Compressed Sensing with {{Fourier}} Measurements},
  author = {Berk, Aaron and Brugiapaglia, Simone and Joshi, Babhru and Plan, Yaniv and Scott, Matthew and Yilmaz, Özgür},
  date = {2022},
  journaltitle = {IEEE Journal on Selected Areas in Information Theory},
  pages = {1--1},
  issn = {2641-8770},
  doi = {10.1109/JSAIT.2022.3220196},
  url = {https://arxiv.org/abs/2207.09340},
  abstract = {In 1, a mathematical framework was developed for compressed sensing guarantees in the setting where the measurement matrix is Gaussian and the signal structure is the range of a generative neural network (GNN). The problem of compressed sensing with GNNs has since been extensively analyzed when the measurement matrix and/or network weights follow a subgaussian distribution. We move beyond the subgaussian assumption, to measurement matrices that are derived by sampling uniformly at random rows of a unitary matrix (including subsampled Fourier measurements as a special case). Specifically, we prove the first known restricted isometry guarantee for generative compressed sensing (GCS) with subsampled isometries, and provide recovery bounds with nearly order-optimal sample complexity, addressing an open problem of 2, p. 10. Recovery efficacy is characterized by the coherence, a new parameter, which measures the interplay between the range of the network and the measurement matrix. Our approach relies on subspace counting arguments and ideas central to high-dimensional probability. Furthermore, we propose a regularization strategy for training GNNs to have favourable coherence with the measurement operator. We provide compelling numerical simulations that support this regularized training strategy: our strategy yields low coherence networks that require fewer measurements for signal recovery. This, together with our theoretical results, supports coherence as a natural quantity for characterizing GCS with subsampled isometries.},
  eventtitle = {{{IEEE Journal}} on {{Selected Areas}} in {{Information Theory}}},
  keywords = {68T07; 60F10; 68P30; 94A08; 94A16,coherence,Coherence,Complexity theory,compressed sensing,Compressed sensing,Computer Science - Information Theory,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Fourier measurements,Generative neural network,Geophysical measurements,Mathematics - Probability,Neural networks,Sensors,Statistics - Machine Learning,subsampled isometry,Weight measurement},
  file = {/Users/matthewscott/Zotero/storage/IXGCPQGU/Berk et al. - 2022 - A coherence parameter characterizing generative co.pdf;/Users/matthewscott/Zotero/storage/UT7FCY9F/Berk et al. - 2022 - A coherence parameter characterizing generative co.pdf;/Users/matthewscott/Zotero/storage/MK6JZXE7/9941342.html;/Users/matthewscott/Zotero/storage/XZWNB9AZ/2207.html}
}

@article{bigotAnalysisBlockSampling2013,
  title = {An {{Analysis}} of {{Block Sampling Strategies}} in {{Compressed Sensing}}},
  author = {Bigot, Jérémie and Boyer, Claire and Weiss, Pierre},
  date = {2013-05-20},
  journaltitle = {IEEE Transactions on Information Theory},
  shortjournal = {IEEE Transactions on Information Theory},
  volume = {62},
  doi = {10.1109/TIT.2016.2524628},
  abstract = {Compressed sensing (CS) is a theory which guarantees the exact recovery of sparse signals from a few number of linear projections. The sampling schemes suggested by current CS theories are often of little relevance since they cannot be implemented on practical acquisition systems. In this paper, we study a new random sampling approach that consists in selecting a set of blocks that are predefined by the application of interest. A typical example is the case where the blocks consist in horizontal lines in the 2D Fourier plane. We provide theoretical results on the number of blocks that are required for exact sparse signal reconstruction in a noise free setting. We illustrate this theory for various sensing matrices appearing in applications such as time-frequency bases. A typical result states that it is sufficient to acquire no more than \$O\textbackslash left(s \textbackslash ln\^2(n) \textbackslash right)\$ lines in the 2D Fourier domain for the perfect reconstruction of an \$s\$-sparse image of size \$\textbackslash sqrt\{n\} \textbackslash times \textbackslash sqrt\{n\}\$ . The proposed results have a large number of potential applications in systems such as magnetic resonance imaging, radio-interferometry or ultra-sound imaging.},
  file = {/Users/matthewscott/Zotero/storage/F4ZMPKCY/Bigot et al. - 2013 - An Analysis of Block Sampling Strategies in Compre.pdf}
}

@article{boguckiSupremaCanonicalWeibull2015,
  title = {Suprema of Canonical {{Weibull}} Processes},
  author = {Bogucki, Robert},
  date = {2015-12},
  journaltitle = {Statistics \& Probability Letters},
  shortjournal = {Statistics \& Probability Letters},
  volume = {107},
  eprint = {1503.06252},
  eprinttype = {arxiv},
  eprintclass = {math},
  pages = {253--263},
  issn = {01677152},
  doi = {10.1016/j.spl.2015.09.002},
  url = {http://arxiv.org/abs/1503.06252},
  urldate = {2023-02-22},
  abstract = {In this note we investigate the problem of bounding the suprema of canonical processes based on r.v.s with tails \$\textbackslash exp\{(-t\^r)\}\$, where \$0},
  keywords = {Mathematics - Probability},
  file = {/Users/matthewscott/Zotero/storage/UUGV53T8/Bogucki - 2015 - Suprema of canonical Weibull processes.pdf;/Users/matthewscott/Zotero/storage/JYNNFERT/1503.html}
}

@inproceedings{boraCompressedSensingUsing2017,
  title = {Compressed {{Sensing}} Using {{Generative Models}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Bora, Ashish and Jalal, Ajil and Price, Eric and Dimakis, Alexandros G.},
  date = {2017-07-17},
  pages = {537--546},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/bora17a.html},
  urldate = {2023-01-07},
  abstract = {The goal of compressed sensing is to estimate a vector from an underdetermined system of noisy linear measurements, by making use of prior knowledge on the structure of vectors in the relevant domain. For almost all results in this literature, the structure is represented by sparsity in a well-chosen basis. We show how to achieve guarantees similar to standard compressed sensing but without employing sparsity at all. Instead, we suppose that vectors lie near the range of a generative model 𝐺:ℝ𝑘→ℝ𝑛G:Rk→RnG: \textbackslash mathbb\{R\}\^k \textbackslash to \textbackslash mathbb\{R\}\^n. Our main theorem is that, if 𝐺GG is 𝐿LL-Lipschitz, then roughly (𝑘log𝐿)O(klog⁡L)\textbackslash mathcal\{O\}(k \textbackslash log L) random Gaussian measurements suffice for an ℓ2/ℓ2ℓ2/ℓ2\textbackslash ell\_2/\textbackslash ell\_2 recovery guarantee. We demonstrate our results using generative models from published variational autoencoder and generative adversarial networks. Our method can use 555-101010x fewer measurements than Lasso for the same accuracy.},
  langid = {english},
  keywords = {🔥,Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/5RLB3G7K/Bora et al. - 2017 - Compressed Sensing using Generative Models.pdf;/Users/matthewscott/Zotero/storage/AAXC6ZGQ/Bora et al. - 2017 - Compressed Sensing using Generative Models.pdf;/Users/matthewscott/Zotero/storage/TH59R468/Bora et al. - 2017 - Compressed Sensing using Generative Models.pdf}
}

@book{boucheronConcentrationInequalitiesNonasymptotic2016,
  title = {Concentration {{Inequalities}}: {{A Nonasymptotic Theory}} of {{Independence}}},
  shorttitle = {Concentration {{Inequalities}}},
  author = {Boucheron, Stephane and Lugosi, Gabor and Massart, Pascal},
  date = {2016-02-27},
  publisher = {{Oxford University Press}},
  location = {{Oxford New York, NY}},
  isbn = {978-0-19-876765-7},
  langid = {english},
  pagetotal = {496},
  file = {/Users/matthewscott/Zotero/storage/LYEW2D4J/3557008720543446793concentration_inequalities.pdf}
}

@article{boydDistributedOptimizationStatistical2010,
  title = {Distributed {{Optimization}} and {{Statistical Learning}} via the {{Alternating Direction Method}} of {{Multipliers}}},
  author = {Boyd, Stephen},
  date = {2010},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {FNT in Machine Learning},
  volume = {3},
  number = {1},
  pages = {1--122},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000016},
  url = {http://www.nowpublishers.com/article/Details/MAL-016},
  urldate = {2022-10-04},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/VG9KUZQF/Boyd - 2010 - Distributed Optimization and Statistical Learning .pdf}
}

@online{boyerCompressedSensingStructured2016,
  title = {Compressed Sensing with Structured Sparsity and Structured Acquisition},
  author = {Boyer, Claire and Bigot, Jérémie and Weiss, Pierre},
  date = {2016-06-13},
  number = {arXiv:1505.01619},
  eprint = {arXiv:1505.01619},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1505.01619},
  url = {http://arxiv.org/abs/1505.01619},
  urldate = {2022-12-17},
  abstract = {Compressed Sensing (CS) is an appealing framework for applications such as Magnetic Resonance Imaging (MRI). However, up-to-date, the sensing schemes suggested by CS theories are made of random isolated measurements, which are usually incompatible with the physics of acquisition. To reflect the physical constraints of the imaging device, we introduce the notion of blocks of measurements: the sensing scheme is not a set of isolated measurements anymore, but a set of groups of measurements which may represent any arbitrary shape (parallel or radial lines for instance). Structured acquisition with blocks of measurements are easy to implement, and provide good reconstruction results in practice. However, very few results exist on the theoretical guarantees of CS reconstructions in this setting. In this paper, we derive new CS results for structured acquisitions and signals satisfying a prior structured sparsity. The obtained results provide a recovery probability of sparse vectors that explicitly depends on their support. Our results are thus support-dependent and offer the possibility for flexible assumptions on the sparsity structure. Moreover, the results are drawing-dependent, since we highlight an explicit dependency between the probability of reconstructing a sparse vector and the way of choosing the blocks of measurements. Numerical simulations show that the proposed theory is faithful to experimental observations.},
  pubstate = {preprint},
  keywords = {Computer Science - Information Theory},
  file = {/Users/matthewscott/Zotero/storage/HA2F2YZE/Boyer et al. - 2016 - Compressed sensing with structured sparsity and st.pdf;/Users/matthewscott/Zotero/storage/8G644AIH/1505.html}
}

@thesis{brugiapagliaCOmpRessedSolvINGSparse2016,
  title = {{{COmpRessed SolvING}}: {{Sparse Approximation}} of {{PDEs}} Based on {{Compressed Sensing}}},
  shorttitle = {{{COmpRessed SolvING}}},
  author = {Brugiapaglia, Simone},
  date = {2016-01-18},
  doi = {10.13140/RG.2.1.4567.1923},
  abstract = {In this thesis, we deal with a new framework for the numerical approximation of partial differential equations which employs main ideas and tools from compressed sensing in a Petrov-Galerkin setting. The goal is to compute an s-sparse approximation with respect to a trial basis of dimension N (with s ≪ N) by picking m ≪ N randomly chosen test functions, and to employ sparse optimization techniques to solve the resulting m × N underdetermined linear system. This approach has been named COmpRessed SolvING (in short, CORSING). First, we carry out an extensive numerical assessment of CORSING on advection-diffusion-reaction equations, both in a one- and a two-dimensional setting, showing that the proposed strategy is able to reduce the computational burden associated with a standard Petrov-Galerkin formulation. Successively, we focus on the theoretical analysis of the method. In particular, we prove recovery error estimates both in expectation and in probability, comparing the error associated with the CORSING solution with the best s-term approximation error. With this aim, we propose a new theoretical framework based on a variant of the classical inf-sup property for sparse vectors, that is named Restricted Inf-Sup Property, and on the concept of local a-coherence, that generalizes the notion of local coherence to bilinear forms in Hilbert spaces. The recovery results and the corresponding hypotheses are then theoretically assessed on one-dimensional advection-diffusion-reaction problems, while in the two-dimensional setting the verification is carried out through numerical tests. Finally, a preliminary application of CORSING to three-dimensional advection-diffusion-reaction equations and to the two-dimensional Stokes problem is also provided.},
  file = {/Users/matthewscott/Zotero/storage/FPCUWF8C/Brugiapaglia - 2016 - COmpRessed SolvING Sparse Approximation of PDEs b.pdf}
}

@online{brugiapagliaInvarianceEncodingsGeneralization2022,
  title = {Invariance, Encodings, and Generalization: Learning Identity Effects with Neural Networks},
  shorttitle = {Invariance, Encodings, and Generalization},
  author = {Brugiapaglia, S. and Liu, M. and Tupper, P.},
  date = {2022-03-02},
  number = {arXiv:2101.08386},
  eprint = {arXiv:2101.08386},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2101.08386},
  urldate = {2022-12-13},
  abstract = {Often in language and other areas of cognition, whether two components of an object are identical or not determines if it is well formed. We call such constraints identity effects. When developing a system to learn well-formedness from examples, it is easy enough to build in an identify effect. But can identity effects be learned from the data without explicit guidance? We provide a framework in which we can rigorously prove that algorithms satisfying simple criteria cannot make the correct inference. We then show that a broad class of learning algorithms including deep feedforward neural networks trained via gradient-based algorithms (such as stochastic gradient descent or the Adam method) satisfy our criteria, dependent on the encoding of inputs. In some broader circumstances we are able to provide adversarial examples that the network necessarily classifies incorrectly. Finally, we demonstrate our theory with computational experiments in which we explore the effect of different input encodings on the ability of algorithms to generalize to novel inputs. This allows us to show similar effects to those predicted by theory for more realistic methods that violate some of the conditions of our theoretical results.},
  pubstate = {preprint},
  keywords = {68Q25; 68R10; 68U05,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/SXJ73BLB/Brugiapaglia et al. - 2022 - Invariance, encodings, and generalization learnin.pdf;/Users/matthewscott/Zotero/storage/RHFA7RHV/2101.html}
}

@unpublished{bubeckUniversalLawRobustness2021,
  title = {A {{Universal Law}} of {{Robustness}} via {{Isoperimetry}}},
  author = {Bubeck, Sébastien and Sellke, Mark},
  date = {2021-10-21},
  eprint = {2105.12806},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2105.12806},
  urldate = {2022-02-27},
  abstract = {Classically, data interpolation with a parametrized model class is possible as long as the number of parameters is larger than the number of equations to be satisfied. A puzzling phenomenon in deep learning is that models are trained with many more parameters than what this classical theory would suggest. We propose a theoretical explanation for this phenomenon. We prove that for a broad class of data distributions and model classes, overparametrization is necessary if one wants to interpolate the data smoothly. Namely we show that smooth interpolation requires \$d\$ times more parameters than mere interpolation, where \$d\$ is the ambient data dimension. We prove this universal law of robustness for any smoothly parametrized function class with polynomial size weights, and any covariate distribution verifying isoperimetry. In the case of two-layers neural networks and Gaussian covariates, this law was conjectured in prior work by Bubeck, Li and Nagaraj. We also give an interpretation of our result as an improved generalization bound for model classes consisting of smooth functions.},
  version = {3},
  keywords = {Computer Science - Machine Learning,Isoperimetry,Learning Theory,Robustness,Statistics - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/Z4GE5EX9/Bubeck and Sellke - 2021 - A Universal Law of Robustness via Isoperimetry.pdf;/Users/matthewscott/Zotero/storage/DC6A7DLL/2105.html}
}

@article{candesSparsityIncoherenceCompressive2007,
  title = {Sparsity and {{Incoherence}} in {{Compressive Sampling}}},
  author = {Candes, Emmanuel and Romberg, Justin},
  date = {2007-06-01},
  journaltitle = {Inverse Problems},
  shortjournal = {Inverse Problems},
  volume = {23},
  number = {3},
  eprint = {math/0611957},
  eprinttype = {arxiv},
  pages = {969--985},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/23/3/008},
  url = {http://arxiv.org/abs/math/0611957},
  urldate = {2022-12-16},
  abstract = {We consider the problem of reconstructing a sparse signal \$x\^0\textbackslash in\textbackslash R\^n\$ from a limited number of linear measurements. Given \$m\$ randomly selected samples of \$U x\^0\$, where \$U\$ is an orthonormal matrix, we show that \$\textbackslash ell\_1\$ minimization recovers \$x\^0\$ exactly when the number of measurements exceeds \textbackslash [ m\textbackslash geq \textbackslash mathrm\{Const\}\textbackslash cdot\textbackslash mu\^2(U)\textbackslash cdot S\textbackslash cdot\textbackslash log n, \textbackslash ] where \$S\$ is the number of nonzero components in \$x\^0\$, and \$\textbackslash mu\$ is the largest entry in \$U\$ properly normalized: \$\textbackslash mu(U) = \textbackslash sqrt\{n\} \textbackslash cdot \textbackslash max\_\{k,j\} |U\_\{k,j\}|\$. The smaller \$\textbackslash mu\$, the fewer samples needed. The result holds for ``most'' sparse signals \$x\^0\$ supported on a fixed (but arbitrary) set \$T\$. Given \$T\$, if the sign of \$x\^0\$ for each nonzero entry on \$T\$ and the observed values of \$Ux\^0\$ are drawn at random, the signal is recovered with overwhelming probability. Moreover, there is a sense in which this is nearly optimal since any method succeeding with the same probability would require just about this many samples.},
  keywords = {Mathematics - Probability,Mathematics - Statistics Theory},
  file = {/Users/matthewscott/Zotero/storage/N9L9I9Y9/Candes and Romberg - 2007 - Sparsity and Incoherence in Compressive Sampling.pdf;/Users/matthewscott/Zotero/storage/UUWHN29E/0611957.html}
}

@article{championInftyWassersteinDistance2008,
  title = {The \$\textbackslash infty\$-{{Wasserstein Distance}}: {{Local Solutions}} and {{Existence}} of {{Optimal Transport Maps}}},
  shorttitle = {The \$\textbackslash infty\$-{{Wasserstein Distance}}},
  author = {Champion, Thierry and De Pascale, Luigi and Juutinen, Petri},
  date = {2008-01},
  journaltitle = {SIAM Journal on Mathematical Analysis},
  shortjournal = {SIAM J. Math. Anal.},
  volume = {40},
  number = {1},
  pages = {1--20},
  issn = {0036-1410},
  doi = {10.1137/07069938X},
  url = {https://epubs.siam.org/doi/abs/10.1137/07069938X},
  urldate = {2022-10-02},
  abstract = {We consider the non-nonlinear optimal transportation problem of minimizing the cost functional \$\textbackslash mathcal\{C\}\_\textbackslash infty(\textbackslash lambda) = \textbackslash operatornamewithlimits\{\textbackslash lambda-ess\textbackslash,sup\}\_\{(x,y) \textbackslash in \textbackslash Omega\^2\} |y-x|\$ in the set of probability measures on \$\textbackslash Omega\^2\$ having prescribed marginals. This corresponds to the question of characterizing the measures that realize the infinite Wasserstein distance. We establish the existence of “local” solutions and characterize this class with the aid of an adequate version of cyclical monotonicity. Moreover, under natural assumptions, we show that local solutions are induced by transport maps.},
  keywords = {49K30,49Q20,infinite cyclical monotonicity,infinite Wasserstein distance,restrictable solutions}
}

@book{changDeepConvolutionalNeural2021,
  title = {Deep {{Convolutional Neural Networks}} with {{Unitary Weights}}},
  author = {Chang, Hao-Yuan and Wang, Kang},
  date = {2021-02-23},
  abstract = {While normalizations aim to fix the exploding and vanishing gradient problem in deep neural networks, they have drawbacks in speed or accuracy because of their dependency on the data set statistics. This work is a comprehensive study of a novel method based on unitary synaptic weights derived from Lie Group to construct intrinsically stable neural systems. Here we show that unitary convolutional neural networks deliver up to 32\% faster inference speeds while maintaining competitive prediction accuracy. Unlike prior arts restricted to square synaptic weights, we expand the unitary networks to weights of any size and dimension.},
  file = {/Users/matthewscott/Zotero/storage/5ZPIQW9Y/Chang and Wang - 2021 - Deep Convolutional Neural Networks with Unitary We.pdf}
}

@book{chengPartialDifferenceEquations1766,
  title = {Partial {{Difference Equations}}},
  author = {Cheng, Sui Sun},
  date = {1766-01-01},
  publisher = {{CRC Press}},
  file = {/Users/matthewscott/Zotero/storage/VT5HXSGR/(Advances in Discrete Mathematics and Applications, 3) Sui Sun Cheng - Partial difference equations-CRC Press (2003).pdf}
}

@book{chikuseStatisticsSpecialManifolds2012,
  title = {Statistics on {{Special Manifolds}}},
  author = {Chikuse, Yasuko},
  date = {2012-11-12},
  eprint = {7lX1BwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  url = {https://link.springer.com/book/10.1007/978-0-387-21540-2},
  abstract = {The special manifolds of interest in this book are the Stiefel manifold and the Grassmann manifold. Formally, the Stiefel manifold Vk,m is the space of k frames in the m-dimensional real Euclidean space Rm, represented by the set of m x k matrices X such that X' X = I , where Ik is the k x k identity matrix, k and the Grassmann manifold Gk,m-k is the space of k-planes (k-dimensional hyperplanes) in Rm. We see that the manifold Pk,m-k of m x m orthogonal projection matrices idempotent of rank k corresponds uniquely to Gk,m-k. This book is concerned with statistical analysis on the manifolds Vk,m and Pk,m-k as statistical sample spaces consisting of matrices. The discussion is carried out on the real spaces so that scalars, vectors, and matrices treated in this book are all real, unless explicitly stated otherwise. For the special case k = 1, the observations from V1,m and G1,m-l are regarded as directed vectors on a unit sphere and as undirected axes or lines, respectively. There exists a large literature of applications of directional statis tics and its statistical analysis, mostly occurring for m = 2 or 3 in practice, in the Earth (or Geological) Sciences, Astrophysics, Medicine, Biology, Meteo rology, Animal Behavior, and many other fields. Examples of observations on the general Grassmann manifold Gk,m-k arise in the signal processing of radar with m elements observing k targets.},
  isbn = {978-0-387-21540-2},
  langid = {english},
  pagetotal = {425},
  keywords = {\#Grassmannian \#Manifold \#Statistics,Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes},
  file = {/Users/matthewscott/Zotero/storage/7CRBM9YF/Statistics on Special Manifolds.pdf;/Users/matthewscott/Zotero/storage/9EXP9MWJ/978-0-387-21540-2.html}
}

@online{childVeryDeepVAEs2021,
  title = {Very {{Deep VAEs Generalize Autoregressive Models}} and {{Can Outperform Them}} on {{Images}}},
  author = {Child, Rewon},
  date = {2021-03-16},
  number = {arXiv:2011.10650},
  eprint = {arXiv:2011.10650},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2011.10650},
  urldate = {2022-12-11},
  abstract = {We present a hierarchical VAE that, for the first time, generates samples quickly while outperforming the PixelCNN in log-likelihood on all natural image benchmarks. We begin by observing that, in theory, VAEs can actually represent autoregressive models, as well as faster, better models if they exist, when made sufficiently deep. Despite this, autoregressive models have historically outperformed VAEs in log-likelihood. We test if insufficient depth explains why by scaling a VAE to greater stochastic depth than previously explored and evaluating it CIFAR-10, ImageNet, and FFHQ. In comparison to the PixelCNN, these very deep VAEs achieve higher likelihoods, use fewer parameters, generate samples thousands of times faster, and are more easily applied to high-resolution images. Qualitative studies suggest this is because the VAE learns efficient hierarchical visual representations. We release our source code and models at https://github.com/openai/vdvae.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/58I6LAYM/Child - 2021 - Very Deep VAEs Generalize Autoregressive Models an.pdf;/Users/matthewscott/Zotero/storage/FCY3CCHB/2011.html}
}

@book{choksiPartialDifferentialEquations2022,
  title = {Partial Differential Equations: A First Course},
  shorttitle = {Partial Differential Equations},
  author = {Choksi, Rustum},
  date = {2022},
  series = {Pure and Applied Undergraduate Texts},
  number = {volume 54},
  publisher = {{American Mathematical Society}},
  location = {{Providence, Rhode Island}},
  abstract = {"The pdf contains a draft title page, draft copyright page and a draft manuscript"--},
  isbn = {978-1-4704-6491-2},
  langid = {english},
  pagetotal = {612},
  keywords = {Differential equations; Partial,Partial differential equations -- Elliptic equations and systems [See also 58J10; 58J20],Partial differential equations -- Equations and systems with constant coefficients [See also 35N05],Partial differential equations -- Equations of mathematical physics and other areas of application [See also 35J05; 35J10; 35K05; 35L05],Partial differential equations -- General first-order equations and systems,Partial differential equations -- General topics,Partial differential equations -- Generalized solutions,Partial differential equations -- Hyperbolic equations and systems [See also 58J45],Partial differential equations -- Parabolic equations and systems [See also 35Bxx; 35Dxx; 35R30; 35R35; 58J35],Partial differential equations -- Representations of solutions,Partial differential equations -- Spectral theory and eigenvalue problems [See also 47Axx; 47Bxx; 47F05]},
  file = {/Users/matthewscott/Zotero/storage/3YI642XD/Choksi - 2022 - Partial differential equations a first course.pdf}
}

@inproceedings{clarksonTighterBoundsRandom2008,
  title = {Tighter Bounds for Random Projections of Manifolds},
  booktitle = {Proceedings of the Twenty-Fourth Annual Symposium on {{Computational}} Geometry  - {{SCG}} '08},
  author = {Clarkson, Kenneth L.},
  date = {2008-06-09},
  pages = {39},
  publisher = {{Association for Computing Machinery}},
  location = {{College Park, MD, USA}},
  doi = {10.1145/1377676.1377685},
  url = {http://portal.acm.org/citation.cfm?doid=1377676.1377685},
  urldate = {2022-08-12},
  abstract = {The Johnson-Lindenstrauss random projection lemma gives a simple way to reduce the dimensionality of a set of points while approximately preserving their pairwise distances. The most direct application of the lemma applies to a finite set of points, but recent work has extended the technique to affine subspaces, curves, and general smooth manifolds. Here the case of random projection of smooth manifolds is considered, and a previous analysis is sharpened, reducing the dependence on such properties as the manifold’s maximum curvature.},
  eventtitle = {The Twenty-Fourth Annual Symposium},
  isbn = {978-1-60558-071-5},
  langid = {english},
  keywords = {johnson-lindenstrauss random projection,Manifolds,Recovery},
  file = {/Users/matthewscott/Zotero/storage/56BPYF55/Clarkson - 2008 - Tighter bounds for random projections of manifolds.pdf}
}

@unpublished{cocolaSignalRecoveryNonExpansive2022,
  title = {Signal {{Recovery}} with {{Non-Expansive Generative Network Priors}}},
  author = {Cocola, Jorio},
  date = {2022-05-21},
  eprint = {2204.13599},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/2204.13599},
  urldate = {2022-05-24},
  abstract = {We study compressive sensing with a deep generative network prior. Initial theoretical guarantees for efficient recovery from compressed linear measurements have been developed for signals in the range of a ReLU network with Gaussian weights and logarithmic expansivity: that is when each layer is larger than the previous one by a logarithmic factor. It was later shown that constant expansivity is sufficient for recovery. It has remained open whether the expansivity can be relaxed, allowing for networks with contractive layers (as often the case of real generators). In this work we answer this question, proving that a signal in the range of a Gaussian generative network can be recovered from few linear measurements provided that the width of the layers is proportional to the input layer size (up to log factors). This condition allows the generative network to have contractive layers. Our result is based on showing that Gaussian matrices satisfy a matrix concentration inequality which we term Range Restricted Weight Distribution Condition (R2WDC) and that weakens the Weight Distribution Condition (WDC) upon which previous theoretical guarantees were based. The WDC has also been used to analyze other signal recovery problems with generative network priors. By replacing the WDC with the R2WDC, we are able to extend previous results for signal recovery with expansive generative network priors to non-expansive ones. We discuss these extensions for phase retrieval, denoising, and spiked matrix recovery.},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Statistics - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/GMZXK67T/Cocola - 2022 - Signal Recovery with Non-Expansive Generative Network Priors.pdf}
}

@book{CodingInformationTheory,
  title = {Coding and {{Information Theory}}},
  url = {https://link.springer.com/book/9780387978123},
  urldate = {2022-08-28},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/GPRMERCX/(Graduate Texts in Mathematics 134) Steven Roman - Coding and Information Theory-Springer (1992).pdf}
}

@book{ComplexAnalysis,
  title = {Complex {{Analysis}}},
  url = {https://link.springer.com/book/10.1007/978-1-4757-3083-8},
  urldate = {2022-12-28},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/AQRRMJ5Z/978-1-4419-7288-0.pdf;/Users/matthewscott/Zotero/storage/GLIU3DW3/Lang S. - Complex Analysis. Volume 103 (2003).djvu;/Users/matthewscott/Zotero/storage/MC32SZXV/Lang S. - Complex Analysis. Volume 103 (2003).djvu;/Users/matthewscott/Zotero/storage/R4V5JNU6/978-1-4757-3083-8.html}
}

@inproceedings{ConcentrationUnboundedMetric,
  title = {Concentration in Unbounded Metric Spaces and Algorithmic Stability},
  location = {{http://proceedings.mlr.press/v32/kontorovicha14.html}}
}

@inproceedings{daneshmandBatchNormalizationOrthogonalizes2021,
  title = {Batch {{Normalization Orthogonalizes Representations}} in {{Deep Random Networks}}},
  author = {Daneshmand, Hadi and Joudaki, Amir and Bach, Francis},
  date = {2021-05-21},
  url = {https://openreview.net/forum?id=_RSgXL8gNnx},
  urldate = {2022-05-24},
  abstract = {We prove that successive batch normalizations, together with random linear layers, incrementally orthogonalize representations of samples.},
  eventtitle = {Advances in {{Neural Information Processing Systems}}},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/PVGKQIJX/Daneshmand et al. - 2021 - Batch Normalization Orthogonalizes Representations.pdf;/Users/matthewscott/Zotero/storage/9IS6BJZD/forum.html}
}

@online{darasIntermediateLayerOptimization2021,
  title = {Intermediate {{Layer Optimization}} for {{Inverse Problems}} Using {{Deep Generative Models}}},
  author = {Daras, Giannis and Dean, Joseph and Jalal, Ajil and Dimakis, Alexandros G.},
  date = {2021-02-15},
  number = {arXiv:2102.07364},
  eprint = {arXiv:2102.07364},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2102.07364},
  url = {http://arxiv.org/abs/2102.07364},
  urldate = {2022-11-10},
  abstract = {We propose Intermediate Layer Optimization (ILO), a novel optimization algorithm for solving inverse problems with deep generative models. Instead of optimizing only over the initial latent code, we progressively change the input layer obtaining successively more expressive generators. To explore the higher dimensional spaces, our method searches for latent codes that lie within a small \$l\_1\$ ball around the manifold induced by the previous layer. Our theoretical analysis shows that by keeping the radius of the ball relatively small, we can improve the established error bound for compressed sensing with deep generative models. We empirically show that our approach outperforms state-of-the-art methods introduced in StyleGAN-2 and PULSE for a wide range of inverse problems including inpainting, denoising, super-resolution and compressed sensing.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Intermediate Layer},
  file = {/Users/matthewscott/Zotero/storage/LSQI2CRU/Daras et al. - 2021 - Intermediate Layer Optimization for Inverse Proble.pdf;/Users/matthewscott/Zotero/storage/DJXC54RL/2102.html}
}

@inproceedings{dharModelingSparseDeviations2018,
  title = {Modeling {{Sparse Deviations}} for {{Compressed Sensing}} Using {{Generative Models}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Dhar, Manik and Grover, Aditya and Ermon, Stefano},
  date = {2018-07-03},
  pages = {1214--1223},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/dhar18a.html},
  urldate = {2022-11-10},
  abstract = {In compressed sensing, a small number of linear measurements can be used to reconstruct an unknown signal. Existing approaches leverage assumptions on the structure of these signals, such as sparsity or the availability of a generative model. A domain-specific generative model can provide a stronger prior and thus allow for recovery with far fewer measurements. However, unlike sparsity-based approaches, existing methods based on generative models guarantee exact recovery only over their support, which is typically only a small subset of the space on which the signals are defined. We propose Sparse-Gen, a framework that allows for sparse deviations from the support set, thereby achieving the best of both worlds by using a domain specific prior and allowing reconstruction over the full space of signals. Theoretically, our framework provides a new class of signals that can be acquired using compressed sensing, reducing classic sparse vector recovery to a special case and avoiding the restrictive support due to a generative model prior. Empirically, we observe consistent improvements in reconstruction accuracy over competing approaches, especially in the more practical setting of transfer compressed sensing where a generative model for a data-rich, source domain aids sensing on a data-scarce, target domain.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {Intermediate Layer},
  file = {/Users/matthewscott/Zotero/storage/YR7GS8MA/Dhar et al. - 2018 - Modeling Sparse Deviations for Compressed Sensing .pdf;/Users/matthewscott/Zotero/storage/ZTW4K9JM/Dhar et al. - 2018 - Modeling Sparse Deviations for Compressed Sensing .pdf}
}

@article{dittmerSingularValuesReLU2020,
  title = {Singular {{Values}} for {{ReLU Layers}}},
  author = {Dittmer, Sören and King, Emily J. and Maass, Peter},
  date = {2020-09},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {31},
  number = {9},
  pages = {3594--3605},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2019.2945113},
  abstract = {Despite their prevalence in neural networks, we still lack a thorough theoretical characterization of rectified linear unit (ReLU) layers. This article aims to further our understanding of ReLU layers by studying how the activation function ReLU interacts with the linear component of the layer and what role this interaction plays in the success of the neural network in achieving its intended task. To this end, we introduce two new tools: ReLU singular values of operators and the Gaussian mean width of operators. By presenting, on the one hand, theoretical justifications, results, and interpretations of these two concepts and, on the other hand, numerical experiments and results of the ReLU singular values and the Gaussian mean width being applied to trained neural networks, we hope to give a comprehensive, singular-value-centric view of ReLU layers. We find that ReLU singular values and the Gaussian mean width do not only enable theoretical insights but also provide one with metrics that seem promising for practical applications. In particular, these measures can be used to distinguish correctly and incorrectly classified data as it traverses the network. We conclude by introducing two tools based on our findings: double layers and harmonic pruning.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Gaussian mean width,Harmonic analysis,Learning systems,Measurement,n-width,neural networks,Neural networks,rectified linear unit (ReLU),singular values,Task analysis,Tools},
  file = {/Users/matthewscott/Zotero/storage/XHNPPC47/Dittmer et al. - 2020 - Singular Values for ReLU Layers.pdf;/Users/matthewscott/Zotero/storage/4C8BB5JP/8891761.html}
}

@article{donohoUncertaintyPrinciplesIdeal2001,
  title = {Uncertainty Principles and Ideal Atomic Decomposition},
  author = {Donoho, D.L. and Huo, X.},
  year = {Nov./2001},
  journaltitle = {IEEE Transactions on Information Theory},
  shortjournal = {IEEE Trans. Inform. Theory},
  volume = {47},
  number = {7},
  pages = {2845--2862},
  issn = {00189448},
  doi = {10.1109/18.959265},
  url = {http://ieeexplore.ieee.org/document/959265/},
  urldate = {2022-12-16},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/LL4C4XMD/Donoho and Huo - 2001 - Uncertainty principles and ideal atomic decomposit.pdf}
}

@online{doshiTransformersExplainedVisually2021,
  title = {Transformers {{Explained Visually}} ({{Part}} 3): {{Multi-head Attention}}, Deep Dive},
  shorttitle = {Transformers {{Explained Visually}} ({{Part}} 3)},
  author = {Doshi, Ketan},
  date = {2021-06-03T03:26:11},
  url = {https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853},
  urldate = {2023-03-01},
  abstract = {A Gentle Guide to the inner workings of Self-Attention, Encoder-Decoder Attention, Attention Score and Masking, in Plain English.},
  langid = {english},
  organization = {{Medium}},
  file = {/Users/matthewscott/Zotero/storage/DE5UZC6F/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853.html}
}

@online{doshiTransformersExplainedVisually2021a,
  title = {Transformers {{Explained Visually}} ({{Part}} 1): {{Overview}} of {{Functionality}}},
  shorttitle = {Transformers {{Explained Visually}} ({{Part}} 1)},
  author = {Doshi, Ketan},
  date = {2021-06-03T03:23:58},
  url = {https://towardsdatascience.com/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452},
  urldate = {2023-03-01},
  abstract = {A Gentle Guide to Transformers for NLP, and why they are better than RNNs, in Plain English. How Attention helps improve performance.},
  langid = {english},
  organization = {{Medium}},
  file = {/Users/matthewscott/Zotero/storage/KMJACXW8/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452.html}
}

@online{DropGNNRandomDropouts,
  title = {{{DropGNN}}: {{Random Dropouts Increase}} the {{Expressiveness}} of {{Graph Neural Networks}}},
  url = {https://nips.cc/virtual/2021/spotlight/26555},
  urldate = {2022-05-24},
  file = {/Users/matthewscott/Zotero/storage/GPHACTTC/26555.html}
}

@book{durrettProbabilityTheoryExamples2010,
  title = {Probability: {{Theory}} and {{Examples}}},
  shorttitle = {Probability},
  author = {Durrett, Rick},
  date = {2010-08-30},
  eprint = {evbGTPhuvSoC},
  eprinttype = {googlebooks},
  publisher = {{Cambridge University Press}},
  abstract = {This classic introduction to probability theory for beginning graduate students covers laws of large numbers, central limit theorems, random walks, martingales, Markov chains, ergodic theorems, and Brownian motion. It is a comprehensive treatment concentrating on the results that are the most useful for applications. Its philosophy is that the best way to learn probability is to see it in action, so there are 200 examples and 450 problems. The fourth edition begins with a short chapter on measure theory to orient readers new to the subject.},
  isbn = {978-1-139-49113-6},
  langid = {english},
  pagetotal = {441},
  keywords = {Mathematics / Probability & Statistics / General},
  file = {/Users/matthewscott/Zotero/storage/XKFNQHT3/Durrett - 2010 - Probability Theory and Examples.pdf}
}

@online{elkinTerminalEmbeddings2016,
  title = {Terminal {{Embeddings}}},
  author = {Elkin, Michael and Filtser, Arnold and Neiman, Ofer},
  date = {2016-03-07},
  number = {arXiv:1603.02321},
  eprint = {arXiv:1603.02321},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1603.02321},
  url = {http://arxiv.org/abs/1603.02321},
  urldate = {2022-12-07},
  abstract = {In this paper we study \{\textbackslash em terminal embeddings\}, in which one is given a finite metric \$(X,d\_X)\$ (or a graph \$G=(V,E)\$) and a subset \$K \textbackslash subseteq X\$ of its points are designated as \{\textbackslash em terminals\}. The objective is to embed the metric into a normed space, while approximately preserving all distances among pairs that contain a terminal. We devise such embeddings in various settings, and conclude that even though we have to preserve \$\textbackslash approx|K|\textbackslash cdot |X|\$ pairs, the distortion depends only on \$|K|\$, rather than on \$|X|\$. We also strengthen this notion, and consider embeddings that approximately preserve the distances between \{\textbackslash em all\} pairs, but provide improved distortion for pairs containing a terminal. Surprisingly, we show that such embeddings exist in many settings, and have optimal distortion bounds both with respect to \$X \textbackslash times X\$ and with respect to \$K \textbackslash times X\$. Moreover, our embeddings have implications to the areas of Approximation and Online Algorithms. In particular, [ALN08] devised an \$\textbackslash tilde\{O\}(\textbackslash sqrt\{\textbackslash log r\})\$-approximation algorithm for sparsest-cut instances with \$r\$ demands. Building on their framework, we provide an \$\textbackslash tilde\{O\}(\textbackslash sqrt\{\textbackslash log |K|\})\$-approximation for sparsest-cut instances in which each demand is incident on one of the vertices of \$K\$ (aka, terminals). Since \$|K| \textbackslash le r\$, our bound generalizes that of [ALN08].},
  pubstate = {preprint},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/Users/matthewscott/Zotero/storage/954355TG/Elkin et al. - 2016 - Terminal Embeddings.pdf;/Users/matthewscott/Zotero/storage/CSK66A3B/1603.html}
}

@online{elkinTerminalEmbeddings2016a,
  title = {Terminal {{Embeddings}}},
  author = {Elkin, Michael and Filtser, Arnold and Neiman, Ofer},
  date = {2016-03-07},
  number = {arXiv:1603.02321},
  eprint = {arXiv:1603.02321},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1603.02321},
  url = {http://arxiv.org/abs/1603.02321},
  urldate = {2023-03-09},
  abstract = {In this paper we study \{\textbackslash em terminal embeddings\}, in which one is given a finite metric \$(X,d\_X)\$ (or a graph \$G=(V,E)\$) and a subset \$K \textbackslash subseteq X\$ of its points are designated as \{\textbackslash em terminals\}. The objective is to embed the metric into a normed space, while approximately preserving all distances among pairs that contain a terminal. We devise such embeddings in various settings, and conclude that even though we have to preserve \$\textbackslash approx|K|\textbackslash cdot |X|\$ pairs, the distortion depends only on \$|K|\$, rather than on \$|X|\$. We also strengthen this notion, and consider embeddings that approximately preserve the distances between \{\textbackslash em all\} pairs, but provide improved distortion for pairs containing a terminal. Surprisingly, we show that such embeddings exist in many settings, and have optimal distortion bounds both with respect to \$X \textbackslash times X\$ and with respect to \$K \textbackslash times X\$. Moreover, our embeddings have implications to the areas of Approximation and Online Algorithms. In particular, [ALN08] devised an \$\textbackslash tilde\{O\}(\textbackslash sqrt\{\textbackslash log r\})\$-approximation algorithm for sparsest-cut instances with \$r\$ demands. Building on their framework, we provide an \$\textbackslash tilde\{O\}(\textbackslash sqrt\{\textbackslash log |K|\})\$-approximation for sparsest-cut instances in which each demand is incident on one of the vertices of \$K\$ (aka, terminals). Since \$|K| \textbackslash le r\$, our bound generalizes that of [ALN08].},
  pubstate = {preprint},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/Users/matthewscott/Zotero/storage/G562D92H/Elkin et al_2016_Terminal Embeddings.pdf;/Users/matthewscott/Zotero/storage/9LBS42LX/1603.html}
}

@article{federerCurvatureMeasures1958,
  title = {Curvature {{Measures}}},
  author = {Federer, Herbert},
  date = {1958},
  eprint = {1993504},
  eprinttype = {jstor},
  url = {https://www.jstor.org/stable/1993504},
  urldate = {2022-12-07},
  file = {/Users/matthewscott/Zotero/storage/IBJE5BHH/Federer - CURVATURE MEASURES(1).pdf;/Users/matthewscott/Zotero/storage/4J3AE9T3/1993504.html}
}

@book{foucartMathematicalIntroductionCompressive2013,
  title = {A {{Mathematical Introduction}} to {{Compressive Sensing}}},
  author = {Foucart, Simon and Rauhut, Holger},
  date = {2013-06-21},
  eprint = {bEUSswEACAAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer New York}},
  abstract = {At the intersection of mathematics, engineering, and computer science sits the thriving field of compressive sensing. Based on the premise that data acquisition and compression can be performed simultaneously, compressive sensing finds applications in imaging, signal processing, and many other domains. In the areas of applied mathematics, electrical engineering, and theoretical computer science, an explosion of research activity has already followed the theoretical results that highlighted the efficiency of the basic principles. The elegant ideas behind these principles are also of independent interest to pure mathematicians.A Mathematical Introduction to Compressive Sensing gives a detailed account of the core theory upon which the field is build. With only moderate prerequisites, it is an excellent textbook for graduate courses in mathematics, engineering, and computer science. It also serves as a reliable resource for practitioners and researchers in these disciplines who want to acquire a careful understanding of the subject. A Mathematical Introduction to Compressive Sensing uses a mathematical perspective to present the core of the theory underlying compressive sensing.},
  isbn = {978-1-4939-0063-3},
  langid = {english},
  pagetotal = {625},
  keywords = {Computers / Computer Science,Computers / Data Science / General,Computers / General,Mathematics / Applied,Mathematics / Discrete Mathematics,Mathematics / Functional Analysis,Technology & Engineering / Electronics / General,Technology & Engineering / Imaging Systems,Technology & Engineering / Telecommunications},
  file = {/Users/matthewscott/Zotero/storage/VEZINICJ/(Applied and Numerical Harmonic Analysis) Simon Foucart, Holger Rauhut (auth.) - A Mathematical Introduction to Compressive Sensing-Birkhäuser Basel (2013).pdf}
}

@article{giryesDeepNeuralNetworks2016,
  title = {Deep {{Neural Networks}} with {{Random Gaussian Weights}}: {{A Universal Classification Strategy}}?},
  shorttitle = {Deep {{Neural Networks}} with {{Random Gaussian Weights}}},
  author = {Giryes, Raja and Sapiro, Guillermo and Bronstein, Alex M.},
  date = {2016-07-01},
  journaltitle = {IEEE Transactions on Signal Processing},
  shortjournal = {IEEE Trans. Signal Process.},
  volume = {64},
  number = {13},
  eprint = {1504.08291},
  eprinttype = {arxiv},
  pages = {3444--3457},
  issn = {1053-587X, 1941-0476},
  doi = {10.1109/TSP.2016.2546221},
  url = {http://arxiv.org/abs/1504.08291},
  urldate = {2022-05-02},
  abstract = {Three important properties of a classification machinery are: (i) the system preserves the core information of the input data; (ii) the training examples convey information about unseen data; and (iii) the system is able to treat differently points from different classes. In this work we show that these fundamental properties are satisfied by the architecture of deep neural networks. We formally prove that these networks with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data. Similar points at the input of the network are likely to have a similar output. The theoretical analysis of deep networks here presented exploits tools used in the compressed sensing and dictionary learning literature, thereby making a formal connection between these important topics. The derived results allow drawing conclusions on the metric learning properties of the network and their relation to its structure, as well as providing bounds on the required size of the training set such that the training examples would represent faithfully the unseen data. The results are validated with state-of-the-art trained networks.},
  keywords = {62M45,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.5.1,Statistics - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/BM86BJ6G/Giryes et al. - 2016 - Deep Neural Networks with Random Gaussian Weights.pdf;/Users/matthewscott/Zotero/storage/PJLFHUJZ/1504.html}
}

@online{gomezFastProvableADMM2019,
  title = {Fast and {{Provable ADMM}} for {{Learning}} with {{Generative Priors}}},
  author = {Gómez, Fabian Latorre and Eftekhari, Armin and Cevher, Volkan},
  date = {2019-07-07},
  number = {arXiv:1907.03343},
  eprint = {arXiv:1907.03343},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1907.03343},
  url = {http://arxiv.org/abs/1907.03343},
  urldate = {2022-10-01},
  abstract = {In this work, we propose a (linearized) Alternating Direction Method-of-Multipliers (ADMM) algorithm for minimizing a convex function subject to a nonconvex constraint. We focus on the special case where such constraint arises from the specification that a variable should lie in the range of a neural network. This is motivated by recent successful applications of Generative Adversarial Networks (GANs) in tasks like compressive sensing, denoising and robustness against adversarial examples. The derived rates for our algorithm are characterized in terms of certain geometric properties of the generator network, which we show hold for feedforward architectures, under mild assumptions. Unlike gradient descent (GD), it can efficiently handle non-smooth objectives as well as exploit efficient partial minimization procedures, thus being faster in many practical scenarios.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning}
}

@online{gozcuLearningBasedCompressiveMRI2018,
  title = {Learning-{{Based Compressive MRI}}},
  author = {Gözcü, Baran and Mahabadi, Rabeeh Karimi and Li, Yen-Huan and Ilıcak, Efe and Çukur, Tolga and Scarlett, Jonathan and Cevher, Volkan},
  date = {2018-05-03},
  number = {arXiv:1805.01266},
  eprint = {arXiv:1805.01266},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1805.01266},
  urldate = {2022-12-15},
  abstract = {In the area of magnetic resonance imaging (MRI), an extensive range of non-linear reconstruction algorithms have been proposed that can be used with general Fourier subsampling patterns. However, the design of these subsampling patterns has typically been considered in isolation from the reconstruction rule and the anatomy under consideration. In this paper, we propose a learning-based framework for optimizing MRI subsampling patterns for a specific reconstruction rule and anatomy, considering both the noiseless and noisy settings. Our learning algorithm has access to a representative set of training signals, and searches for a sampling pattern that performs well on average for the signals in this set. We present a novel parameter-free greedy mask selection method, and show it to be effective for a variety of reconstruction rules and performance metrics. Moreover we also support our numerical findings by providing a rigorous justification of our framework via statistical learning theory.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/matthewscott/Zotero/storage/X5NRZZDG/Gözcü et al. - 2018 - Learning-Based Compressive MRI.pdf;/Users/matthewscott/Zotero/storage/J7MD6VND/1805.html}
}

@online{GPTTransformersJl,
  title = {{{GPT}} · {{Transformers}}.Jl},
  url = {https://chengchingwen.github.io/Transformers.jl/dev/gpt/},
  urldate = {2022-11-27},
  file = {/Users/matthewscott/Zotero/storage/YMN66SLJ/gpt.html}
}

@article{gregorLearningFastApproximations,
  title = {Learning {{Fast Approximations}} of {{Sparse Coding}}},
  author = {Gregor, Karol and LeCun, Yann},
  pages = {8},
  abstract = {In Sparse Coding (SC), input vectors are reconstructed using a sparse linear combination of basis vectors. SC has become a popular method for extracting features from data. For a given input, SC minimizes a quadratic reconstruction error with an L1 penalty term on the code. The process is often too slow for applications such as real-time pattern recognition. We proposed two versions of a very fast algorithm that produces approximate estimates of the sparse code that can be used to compute good visual features, or to initialize exact iterative algorithms. The main idea is to train a non-linear, feed-forward predictor with a specific architecture and a fixed depth to produce the best possible approximation of the sparse code. A version of the method, which can be seen as a trainable version of Li and Osher’s coordinate descent method, is shown to produce approximate solutions with 10 times less computation than Li and Osher’s for the same approximation error. Unlike previous proposals for sparse code predictors, the system allows a kind of approximate “explaining away” to take place during inference. The resulting predictor is differentiable and can be included into globallytrained recognition systems.},
  langid = {english},
  keywords = {Algorithm Unfolding},
  file = {/Users/matthewscott/Zotero/storage/SLTNTK3B/Gregor and LeCun - Learning Fast Approximations of Sparse Coding.pdf}
}

@inproceedings{guImageProcessingUsing2020,
  title = {Image {{Processing Using Multi-Code GAN Prior}}},
  author = {Gu, Jinjin and Shen, Yujun and Zhou, Bolei},
  date = {2020},
  pages = {3012--3021},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Gu_Image_Processing_Using_Multi-Code_GAN_Prior_CVPR_2020_paper.html},
  urldate = {2022-11-10},
  abstract = {Despite the success of Generative Adversarial Networks (GANs) in image synthesis, applying trained GAN models to real image processing remains challenging. Previous methods typically invert a target image back to the latent space either by back-propagation or by learning an additional encoder. However, the reconstructions from both of the methods are far from ideal. In this work, we propose a novel approach, called mGANprior, to incorporate the well-trained GANs as effective prior to a variety of image processing tasks. In particular, we employ multiple latent codes to generate multiple feature maps at some intermediate layer of the generator, then compose them with adaptive channel importance to recover the input image. Such an over-parameterization of the latent space significantly improves the image reconstruction quality, outperforming existing competitors. The resulting high-fidelity image reconstruction enables the trained GAN models as prior to many real-world applications, such as image colorization, super-resolution, image inpainting, and semantic manipulation. We further analyze the properties of the layer-wise representation learned by GAN models and shed light on what knowledge each layer is capable of representing.},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  keywords = {Intermediate Layer},
  file = {/Users/matthewscott/Zotero/storage/ZIU9UR5R/Gu et al. - 2020 - Image Processing Using Multi-Code GAN Prior.pdf;/Users/matthewscott/Zotero/storage/WN9P54XQ/Gu_Image_Processing_Using_Multi-Code_GAN_Prior_CVPR_2020_paper.html}
}

@online{gunnRegularizedTrainingIntermediate2022,
  title = {Regularized {{Training}} of {{Intermediate Layers}} for {{Generative Models}} for {{Inverse Problems}}},
  author = {Gunn, Sean and Cocola, Jorio and Hand, Paul},
  date = {2022-03-08},
  number = {arXiv:2203.04382},
  eprint = {arXiv:2203.04382},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2203.04382},
  url = {http://arxiv.org/abs/2203.04382},
  urldate = {2022-11-10},
  abstract = {Generative Adversarial Networks (GANs) have been shown to be powerful and flexible priors when solving inverse problems. One challenge of using them is overcoming representation error, the fundamental limitation of the network in representing any particular signal. Recently, multiple proposed inversion algorithms reduce representation error by optimizing over intermediate layer representations. These methods are typically applied to generative models that were trained agnostic of the downstream inversion algorithm. In our work, we introduce a principle that if a generative model is intended for inversion using an algorithm based on optimization of intermediate layers, it should be trained in a way that regularizes those intermediate layers. We instantiate this principle for two notable recent inversion algorithms: Intermediate Layer Optimization and the Multi-Code GAN prior. For both of these inversion algorithms, we introduce a new regularized GAN training algorithm and demonstrate that the learned generative model results in lower reconstruction errors across a wide range of under sampling ratios when solving compressed sensing, inpainting, and super-resolution problems.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Intermediate Layer},
  file = {/Users/matthewscott/Zotero/storage/GL25CPHI/Gunn et al. - 2022 - Regularized Training of Intermediate Layers for Ge.pdf;/Users/matthewscott/Zotero/storage/S7NBMN5T/2203.html}
}

@online{haoBootstrappingUpperConfidence2019,
  title = {Bootstrapping {{Upper Confidence Bound}}},
  author = {Hao, Botao and Abbasi-Yadkori, Yasin and Wen, Zheng and Cheng, Guang},
  date = {2019-06-12},
  doi = {10.48550/arXiv.1906.05247},
  url = {https://arxiv.org/abs/1906.05247v3},
  urldate = {2023-02-17},
  abstract = {Upper Confidence Bound (UCB) method is arguably the most celebrated one used in online decision making with partial information feedback. Existing techniques for constructing confidence bounds are typically built upon various concentration inequalities, which thus lead to over-exploration. In this paper, we propose a non-parametric and data-dependent UCB algorithm based on the multiplier bootstrap. To improve its finite sample performance, we further incorporate second-order correction into the above construction. In theory, we derive both problem-dependent and problem-independent regret bounds for multi-armed bandits under a much weaker tail assumption than the standard sub-Gaussianity. Numerical results demonstrate significant regret reductions by our method, in comparison with several baselines in a range of multi-armed and linear bandit problems.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {/Users/matthewscott/Zotero/storage/9RL672Z2/Hao et al. - 2019 - Bootstrapping Upper Confidence Bound.pdf}
}

@inproceedings{hegdeAlgorithmicAspectsInverse2018,
  title = {Algorithmic {{Aspects}} of {{Inverse Problems Using Generative Models}}},
  booktitle = {2018 56th {{Annual Allerton Conference}} on {{Communication}}, {{Control}}, and {{Computing}} ({{Allerton}})},
  author = {Hegde, Chinmay},
  date = {2018-10},
  pages = {166--172},
  doi = {10.1109/ALLERTON.2018.8636031},
  abstract = {The traditional approach of hand-crafting priors (such as sparsity) for solving inverse problems is slowly being replaced by the use of richer learned priors (such as those modeled by generative adversarial networks, or GANs). In this work, we study the algorithmic aspects of such a learning-based approach from a theoretical perspective. For certain generative network architectures, we establish a simple non-convex algorithmic approach that (a) theoretically enjoys linear convergence guarantees for certain inverse problems, and (b) empirically improves upon conventional techniques such as back-propagation. We also propose an extension of our approach that can handle model mismatch (i.e., situations where the generative network prior is not exactly applicable.) Together, our contributions serve as further building blocks towards an algorithmic theory of generative models in inverse problems.},
  eventtitle = {2018 56th {{Annual Allerton Conference}} on {{Communication}}, {{Control}}, and {{Computing}} ({{Allerton}})},
  keywords = {Analytical models,Computational modeling,Convergence,Gallium nitride,Inverse problems,Neural networks,Optimization}
}

@online{hendrycksGaussianErrorLinear2020,
  title = {Gaussian {{Error Linear Units}} ({{GELUs}})},
  author = {Hendrycks, Dan and Gimpel, Kevin},
  date = {2020-07-08},
  number = {arXiv:1606.08415},
  eprint = {arXiv:1606.08415},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1606.08415},
  urldate = {2022-11-27},
  abstract = {We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU activation function is \$x\textbackslash Phi(x)\$, where \$\textbackslash Phi(x)\$ the standard Gaussian cumulative distribution function. The GELU nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLUs (\$x\textbackslash mathbf\{1\}\_\{x{$>$}0\}\$). We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all considered computer vision, natural language processing, and speech tasks.},
  pubstate = {preprint},
  version = {3},
  keywords = {activation,Computer Science - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/RYBLVD4U/Hendrycks and Gimpel - 2020 - Gaussian Error Linear Units (GELUs).pdf;/Users/matthewscott/Zotero/storage/UYCMZKQR/Hendrycks and Gimpel - 2020 - Gaussian Error Linear Units (GELUs).pdf;/Users/matthewscott/Zotero/storage/IM4ISVP5/1606.html}
}

@article{hitczenkoMomentInequalitiesSums1997,
  title = {Moment Inequalities for Sums of Certain Independent Symmetric Random Variables},
  author = {Hitczenko, P. and Montgomery-Smith, S. and Oleszkiewicz, K.},
  date = {1997},
  journaltitle = {Studia Mathematica},
  url = {https://www.semanticscholar.org/paper/Moment-inequalities-for-sums-of-certain-independent-Hitczenko-Montgomery-Smith/bc02be3b37f22dcf1978ddb191dda3153ee880e0},
  urldate = {2023-02-22},
  abstract = {This paper gives upper and lower bounds for moments of sums of independent random variables (Xk) which satisfy the condition that P (|X|k ≥ t) = exp(−Nk(t)), where Nk are concave functions. As a consequence we obtain precise information about the tail probabilities of linear combinations of independent random variables for which N(t) = |t| for some fixed 0 {$<$} r ≤ 1. This complements work of Gluskin and Kwapień who have done the same for convex functions N .},
  file = {/Users/matthewscott/Zotero/storage/9GWV6DDI/Hitczenko et al. - 1997 - Moment inequalities for sums of certain independen.pdf}
}

@article{husseinImageAdaptiveGANBased2020,
  title = {Image-{{Adaptive GAN Based Reconstruction}}},
  author = {Hussein, Shady Abu and Tirer, Tom and Giryes, Raja},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {04},
  pages = {3121--3129},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i04.5708},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/5708},
  urldate = {2022-11-10},
  abstract = {In the recent years, there has been a significant improvement in the quality of samples produced by (deep) generative models such as variational auto-encoders and generative adversarial networks. However, the representation capabilities of these methods still do not capture the full distribution for complex classes of images, such as human faces. This deficiency has been clearly observed in previous works that use pre-trained generative models to solve imaging inverse problems. In this paper, we suggest to mitigate the limited representation capabilities of generators by making them image-adaptive and enforcing compliance of the restoration with the observations via back-projections. We empirically demonstrate the advantages of our proposed approach for image super-resolution and compressed sensing.},
  issue = {04},
  langid = {english},
  keywords = {Intermediate Layer},
  file = {/Users/matthewscott/Zotero/storage/KUP4T6DG/Hussein et al. - 2020 - Image-Adaptive GAN Based Reconstruction.pdf}
}

@article{ishiharaConstructiveProofMinimax2017,
  title = {A Constructive Proof of the Minimax Theorem},
  author = {Ishihara, Hajime},
  date = {2017},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/4PHR4SHL/Ishihara - A constructive proof of the minimax theorem.pdf}
}

@online{iwenOuterBiLipschitzExtensions2022,
  title = {On {{Outer Bi-Lipschitz Extensions}} of {{Linear Johnson-Lindenstrauss Embeddings}} of {{Low-Dimensional Submanifolds}} of \$\textbackslash mathbb\{\vphantom\}{{R}}\vphantom\{\}\^{{N}}\$},
  author = {Iwen, Mark A. and Roach, Mark Philip},
  date = {2022-06-07},
  number = {arXiv:2206.03376},
  eprint = {arXiv:2206.03376},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2206.03376},
  url = {http://arxiv.org/abs/2206.03376},
  urldate = {2022-12-07},
  abstract = {Let \$\textbackslash mathcal\{M\}\$ be a compact \$d\$-dimensional submanifold of \$\textbackslash mathbb\{R\}\^N\$ with reach \$\textbackslash tau\$ and volume \$V\_\{\textbackslash mathcal M\}\$. Fix \$\textbackslash epsilon \textbackslash in (0,1)\$. In this paper we prove that a nonlinear function \$f: \textbackslash mathbb\{R\}\^N \textbackslash rightarrow \textbackslash mathbb\{R\}\^\{m\}\$ exists with \$m \textbackslash leq C \textbackslash left(d / \textbackslash epsilon\^2 \textbackslash right) \textbackslash log \textbackslash left(\textbackslash frac\{\textbackslash sqrt[d]\{V\_\{\textbackslash mathcal M\}\}\}\{\textbackslash tau\} \textbackslash right)\$ such that \$\$(1 - \textbackslash epsilon) \textbackslash | \{\textbackslash bf x\} - \{\textbackslash bf y\} \textbackslash |\_2 \textbackslash leq \textbackslash left\textbackslash | f(\{\textbackslash bf x\}) - f(\{\textbackslash bf y\}) \textbackslash right\textbackslash |\_2 \textbackslash leq (1 + \textbackslash epsilon) \textbackslash | \{\textbackslash bf x\} - \{\textbackslash bf y\} \textbackslash |\_2\$\$ holds for all \$\{\textbackslash bf x\} \textbackslash in \textbackslash mathcal\{M\}\$ and \$\{\textbackslash bf y\} \textbackslash in \textbackslash mathbb\{R\}\^N\$. In effect, \$f\$ not only serves as a bi-Lipschitz function from \$\textbackslash mathcal\{M\}\$ into \$\textbackslash mathbb\{R\}\^\{m\}\$ with bi-Lipschitz constants close to one, but also approximately preserves all distances from points not in \$\textbackslash mathcal\{M\}\$ to all points in \$\textbackslash mathcal\{M\}\$ in its image. Furthermore, the proof is constructive and yields an algorithm which works well in practice. In particular, it is empirically demonstrated herein that such nonlinear functions allow for more accurate compressive nearest neighbor classification than standard linear Johnson-Lindenstrauss embeddings do in practice.},
  pubstate = {preprint},
  keywords = {51F30; 65D18; 68R12,Computer Science - Computational Geometry,Computer Science - Machine Learning,Mathematics - Numerical Analysis},
  file = {/Users/matthewscott/Zotero/storage/V39FPLYM/Iwen and Roach - 2022 - On Outer Bi-Lipschitz Extensions of Linear Johnson.pdf;/Users/matthewscott/Zotero/storage/UWW82Y9D/2206.html}
}

@misc{iwenPresentationMILDTerminal2022,
  title = {Presentation {{MILD Terminal Embeddings}}},
  author = {Iwen, Mark},
  date = {2022},
  file = {/Users/matthewscott/Zotero/storage/IN49JLRB/Iwen - 2022 - Presentation MILD Terminal Embeddings.pdf}
}

@online{jalalInstanceOptimalCompressedSensing2021,
  title = {Instance-{{Optimal Compressed Sensing}} via {{Posterior Sampling}}},
  author = {Jalal, Ajil and Karmalkar, Sushrut and Dimakis, Alexandros G. and Price, Eric},
  date = {2021-06-21},
  number = {arXiv:2106.11438},
  eprint = {arXiv:2106.11438},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2106.11438},
  url = {http://arxiv.org/abs/2106.11438},
  urldate = {2022-10-01},
  abstract = {We characterize the measurement complexity of compressed sensing of signals drawn from a known prior distribution, even when the support of the prior is the entire space (rather than, say, sparse vectors). We show for Gaussian measurements and \textbackslash emph\{any\} prior distribution on the signal, that the posterior sampling estimator achieves near-optimal recovery guarantees. Moreover, this result is robust to model mismatch, as long as the distribution estimate (e.g., from an invertible generative model) is close to the true distribution in Wasserstein distance. We implement the posterior sampling estimator for deep generative priors using Langevin dynamics, and empirically find that it produces accurate estimates with more diversity than MAP.},
  pubstate = {preprint},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/6KTBF82U/2106.11438.pdf}
}

@unpublished{jalalRobustCompressedSensing2021,
  title = {Robust {{Compressed Sensing MRI}} with {{Deep Generative Priors}}},
  author = {Jalal, Ajil and Arvinte, Marius and Daras, Giannis and Price, Eric and Dimakis, Alexandros G. and Tamir, Jonathan I.},
  date = {2021-12-06},
  eprint = {2108.01368},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2108.01368},
  urldate = {2022-03-25},
  abstract = {The CSGM framework (Bora-Jalal-Price-Dimakis'17) has shown that deep generative priors can be powerful tools for solving inverse problems. However, to date this framework has been empirically successful only on certain datasets (for example, human faces and MNIST digits), and it is known to perform poorly on out-of-distribution samples. In this paper, we present the first successful application of the CSGM framework on clinical MRI data. We train a generative prior on brain scans from the fastMRI dataset, and show that posterior sampling via Langevin dynamics achieves high quality reconstructions. Furthermore, our experiments and theory show that posterior sampling is robust to changes in the ground-truth distribution and measurement process. Our code and models are available at: \textbackslash url\{https://github.com/utcsilab/csgm-mri-langevin\}.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Theory,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/HJ95NWUG/Jalal et al. - 2021 - Robust Compressed Sensing MRI with Deep Generative.pdf;/Users/matthewscott/Zotero/storage/FJVMT4Z5/2108.html}
}

@inproceedings{jalalRobustCompressedSensing2021a,
  title = {Robust {{Compressed Sensing MR Imaging}} with {{Deep Generative Priors}}},
  author = {Jalal, Ajil and Arvinte, Marius and Daras, Giannis and Price, Eric and Dimakis, Alex and Tamir, Jonathan},
  date = {2021-10-19},
  url = {https://openreview.net/forum?id=igjxsgnvZPd},
  urldate = {2022-05-24},
  abstract = {Posterior sampling using score-based models is competitive with SOTA methods on fastMRI, and is robust to sampling and anatomy shift},
  eventtitle = {{{NeurIPS}} 2021 {{Workshop}} on {{Deep Learning}} and {{Inverse Problems}}},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/95XRIMNH/Jalal et al. - 2021 - Robust Compressed Sensing MR Imaging with Deep Gen.pdf;/Users/matthewscott/Zotero/storage/E5ZGN9PY/forum.html}
}

@unpublished{jeongSubGaussianMatricesSets2021,
  title = {Sub-{{Gaussian Matrices}} on {{Sets}}: {{Optimal Tail Dependence}} and {{Applications}}},
  shorttitle = {Sub-{{Gaussian Matrices}} on {{Sets}}},
  author = {Jeong, Halyun and Li, Xiaowei and Plan, Yaniv and Yılmaz, Özgür},
  date = {2021-01-20},
  eprint = {2001.10631},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2001.10631},
  urldate = {2022-02-28},
  abstract = {Random linear mappings are widely used in modern signal processing, compressed sensing and machine learning. These mappings may be used to embed the data into a significantly lower dimension while at the same time preserving useful information. This is done by approximately preserving the distances between data points, which are assumed to belong to \$\textbackslash mathbb\{R\}\^n\$. Thus, the performance of these mappings is usually captured by how close they are to an isometry on the data. Gaussian linear mappings have been the object of much study, while the sub-Gaussian settings is not yet fully understood. In the latter case, the performance depends on the sub-Gaussian norm of the rows. In many applications, e.g., compressed sensing, this norm may be large, or even growing with dimension, and thus it is important to characterize this dependence. We study when a sub-Gaussian matrix can become a near isometry on a set, show that previous best known dependence on the sub-Gaussian norm was sub-optimal, and present the optimal dependence. Our result not only answers a remaining question posed by Liaw, Mehrabian, Plan and Vershynin in 2017, but also generalizes their work. We also develop a new Bernstein type inequality for sub-exponential random variables, and a new Hanson-Wright inequality for quadratic forms of sub-Gaussian random variables, in both cases improving the bounds in the sub-Gaussian regime under moment constraints. Finally, we illustrate popular applications such as Johnson-Lindenstrauss embeddings, null space property for 0-1 matrices, randomized sketches and blind demodulation, whose theoretical guarantees can be improved by our results (in the sub-Gaussian case).},
  keywords = {Computer Science - Information Theory,Mathematics - Statistics Theory,Random Matrix Theory,Statistics - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/P4IB77D7/Jeong et al. - 2021 - Sub-Gaussian Matrices on Sets Optimal Tail Depend.pdf;/Users/matthewscott/Zotero/storage/J5YPICCB/2001.html}
}

@inproceedings{jiEarlystoppedNeuralNetworks2021,
  title = {Early-Stopped Neural Networks Are Consistent},
  author = {Ji, Ziwei and Li, Justin D. and Telgarsky, Matus},
  date = {2021-05-21},
  url = {https://openreview.net/forum?id=rMKTq-ca0qu},
  urldate = {2022-05-24},
  abstract = {For general classification problems, including those with noise, gradient descent with early stopping on shallow ReLU networks achieves the optimal risk amongst all measurable predictors},
  eventtitle = {Advances in {{Neural Information Processing Systems}}},
  langid = {english},
  keywords = {\#EarlyStopping},
  file = {/Users/matthewscott/Zotero/storage/U7I9NJ8R/Ji et al. - 2021 - Early-stopped neural networks are consistent.pdf;/Users/matthewscott/Zotero/storage/NR9I2RUK/forum.html}
}

@online{Kamath2020powerPdf,
  title = {Kamath2020power.Pdf},
  url = {https://www.dropbox.com/s/ey6nzyxfriwj55h/kamath2020power.pdf?dl=0},
  urldate = {2023-03-06},
  abstract = {Shared with Dropbox},
  langid = {english},
  organization = {{Dropbox}},
  file = {/Users/matthewscott/Zotero/storage/LZI5UC94/kamath2020power.html}
}

@online{kingmaAutoEncodingVariationalBayes2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2014-05-01},
  number = {arXiv:1312.6114},
  eprint = {arXiv:1312.6114},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1312.6114},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2022-12-11},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/VJDV94CF/Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf;/Users/matthewscott/Zotero/storage/Q7UZNMSI/1312.html}
}

@article{kingmaIntroductionVariationalAutoencoders2019,
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2019},
  journaltitle = {Foundations and Trends® in Machine Learning},
  shortjournal = {FNT in Machine Learning},
  volume = {12},
  number = {4},
  eprint = {1906.02691},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {307--392},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000056},
  url = {http://arxiv.org/abs/1906.02691},
  urldate = {2022-12-11},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/BHCS5URS/Kingma and Welling - 2019 - An Introduction to Variational Autoencoders.pdf;/Users/matthewscott/Zotero/storage/PADYB9LG/1906.html}
}

@unpublished{kontorovichConcentrationUnboundedMetric2013,
  title = {Concentration in Unbounded Metric Spaces and Algorithmic Stability},
  author = {Kontorovich, Aryeh},
  date = {2013-09-04},
  eprint = {1309.1007},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  url = {http://arxiv.org/abs/1309.1007},
  urldate = {2022-04-18},
  abstract = {We prove an extension of McDiarmid's inequality for metric spaces with unbounded diameter. To this end, we introduce the notion of the \{\textbackslash em subgaussian diameter\}, which is a distribution-dependent refinement of the metric diameter. Our technique provides an alternative approach to that of Kutin and Niyogi's method of weakly difference-bounded functions, and yields nontrivial, dimension-free results in some interesting cases where the former does not. As an application, we give apparently the first generalization bound in the algorithmic stability setting that holds for unbounded loss functions. We furthermore extend our concentration inequality to strongly mixing processes.},
  version = {1},
  keywords = {60D99,Computer Science - Machine Learning,Mathematics - Functional Analysis,Mathematics - Probability},
  file = {/Users/matthewscott/Zotero/storage/JREX7XXJ/1309.html}
}

@article{krahmerStableRobustSampling2014,
  title = {Stable and {{Robust Sampling Strategies}} for {{Compressive Imaging}}},
  author = {Krahmer, Felix and Ward, Rachel},
  date = {2014-02},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {23},
  number = {2},
  pages = {612--622},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2013.2288004},
  url = {http://ieeexplore.ieee.org/document/6651836/},
  urldate = {2022-12-15},
  abstract = {In many signal processing applications, one wishes to acquire images that are sparse in transform domains such as spatial finite differences or wavelets using frequency domain samples. For such applications, overwhelming empirical evidence suggests that superior image reconstruction can be obtained through variable density sampling strategies that concentrate on lower frequencies. The wavelet and Fourier transform domains are not incoherent because low-order wavelets and low-order frequencies are correlated, so compressive sensing theory does not immediately imply sampling strategies and reconstruction guarantees. In this paper, we turn to a more refined notion of coherence—the so-called local coherence—measuring for each sensing vector separately how correlated it is to the sparsity basis. For Fourier measurements and Haar wavelet sparsity, the local coherence can be controlled and bounded explicitly, so for matrices comprised of frequencies sampled from a suitable inverse square power-law density, we can prove the restricted isometry property with near-optimal embedding dimensions. Consequently, the variable-density sampling strategy we provide allows for image reconstructions that are stable to sparsity defects and robust to measurement noise. Our results cover both reconstruction by 1-minimization and total variation minimization. The local coherence framework developed in this paper should be of independent interest, as it implies that for optimal sparse recovery results, it suffices to have bounded average coherence from sensing basis to sparsity basis—as opposed to bounded maximal coherence—as long as the sampling strategy is adapted accordingly.},
  langid = {english},
  keywords = {\#Unveven Sampling,94A08; 68U10; 65D18; 92C55,Coherence,Compressed sensing,Compressive imaging,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Information Theory,Extraterrestrial measurements,frequency,Frequency measurement,Image reconstruction,incoherence,local coherence,Mathematics - Numerical Analysis,Noise measurement,variable density sampling,Vectors},
  file = {/Users/matthewscott/Zotero/storage/BCGNVA96/Krahmer and Ward - 2014 - Stable and Robust Sampling Strategies for Compress.pdf;/Users/matthewscott/Zotero/storage/KUGS8MCS/Krahmer and Ward - 2014 - Stable and Robust Sampling Strategies for Compress.pdf;/Users/matthewscott/Zotero/storage/KY7FJANJ/6651836.html;/Users/matthewscott/Zotero/storage/YGIBPHCI/1210.html}
}

@article{latalaEstimationMomentsSums1997,
  title = {Estimation of {{Moments}} of {{Sums}} of {{Independent Real Random Variables}}},
  author = {Latala, Rafal},
  date = {1997},
  journaltitle = {The Annals of Probability},
  volume = {25},
  number = {3},
  eprint = {2959569},
  eprinttype = {jstor},
  pages = {1502--1513},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0091-1798},
  url = {https://www.jstor.org/stable/2959569},
  urldate = {2023-03-22},
  abstract = {For the sum S = ∑ Xi of a sequence (Xi) of independent symmetric (or nonnegative) random variables, we give lower and upper estimates of moments of S. The estimates are exact, up to some universal constants, and extend the previous results for particular types of variables Xi.},
  file = {/Users/matthewscott/Zotero/storage/D28E2WPT/Latala_1997_Estimation of Moments of Sums of Independent Real Random Variables.pdf}
}

@misc{LayersClassification,
  title = {Layers and {{Classification}}},
  file = {/Users/matthewscott/Zotero/storage/4XHATJXP/Layers and Classification.pdf}
}

@unpublished{leeDeepNeuralNetworks2018,
  title = {Deep {{Neural Networks}} as {{Gaussian Processes}}},
  author = {Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  date = {2018-03-02},
  eprint = {1711.00165},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1711.00165},
  url = {http://arxiv.org/abs/1711.00165},
  urldate = {2022-12-11},
  abstract = {It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/Q66NXPQL/Lee et al. - 2018 - Deep Neural Networks as Gaussian Processes.pdf;/Users/matthewscott/Zotero/storage/WYYZZ2TB/1711.html}
}

@article{leeWideNeuralNetworks2019,
  title = {Wide Neural Networks of Any Depth Evolve as Linear Models under Gradient Descent},
  author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  date = {2019},
  journaltitle = {Advances in neural information processing systems},
  volume = {32}
}

@article{linSurveyTransformers2022,
  title = {A Survey of Transformers},
  author = {Lin, Tianyang and Wang, Yuxin and Liu, Xiangyang and Qiu, Xipeng},
  date = {2022-01-01},
  journaltitle = {AI Open},
  shortjournal = {AI Open},
  volume = {3},
  pages = {111--132},
  issn = {2666-6510},
  doi = {10.1016/j.aiopen.2022.10.001},
  url = {https://www.sciencedirect.com/science/article/pii/S2666651022000146},
  urldate = {2022-11-27},
  abstract = {Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.},
  langid = {english},
  keywords = {Deep learning,Pre-trained models,Self-attention,Transformer,Transformers},
  file = {/Users/matthewscott/Zotero/storage/5PIZYQY8/Lin et al. - 2022 - A survey of transformers.pdf;/Users/matthewscott/Zotero/storage/KMUDWCYS/S2666651022000146.html}
}

@unpublished{liuDeepLearningTheory2019,
  title = {Deep {{Learning Theory Review}}: {{An Optimal Control}} and {{Dynamical Systems Perspective}}},
  shorttitle = {Deep {{Learning Theory Review}}},
  author = {Liu, Guan-Horng and Theodorou, Evangelos A.},
  date = {2019-09-28},
  eprint = {1908.10920},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/1908.10920},
  urldate = {2022-03-25},
  abstract = {Attempts from different disciplines to provide a fundamental understanding of deep learning have advanced rapidly in recent years, yet a unified framework remains relatively limited. In this article, we provide one possible way to align existing branches of deep learning theory through the lens of dynamical system and optimal control. By viewing deep neural networks as discrete-time nonlinear dynamical systems, we can analyze how information propagates through layers using mean field theory. When optimization algorithms are further recast as controllers, the ultimate goal of training processes can be formulated as an optimal control problem. In addition, we can reveal convergence and generalization properties by studying the stochastic dynamics of optimization algorithms. This viewpoint features a wide range of theoretical study from information bottleneck to statistical physics. It also provides a principled way for hyper-parameter tuning when optimal control theory is introduced. Our framework fits nicely with supervised learning and can be extended to other learning problems, such as Bayesian learning, adversarial training, and specific forms of meta learning, without efforts. The review aims to shed lights on the importance of dynamics and optimal control when developing deep learning theory.},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/3AJ8XMCE/Liu and Theodorou - 2019 - Deep Learning Theory Review An Optimal Control an.pdf;/Users/matthewscott/Zotero/storage/J3YZE3JJ/1908.html}
}

@online{mahabadiNonlinearDimensionReduction2018,
  title = {Nonlinear {{Dimension Reduction}} via {{Outer Bi-Lipschitz Extensions}}},
  author = {Mahabadi, Sepideh and Makarychev, Konstantin and Makarychev, Yury and Razenshteyn, Ilya},
  date = {2018-11-08},
  number = {arXiv:1811.03591},
  eprint = {arXiv:1811.03591},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1811.03591},
  url = {http://arxiv.org/abs/1811.03591},
  urldate = {2022-12-07},
  abstract = {We introduce and study the notion of an outer bi-Lipschitz extension of a map between Euclidean spaces. The notion is a natural analogue of the notion of a Lipschitz extension of a Lipschitz map. We show that for every map \$f\$ there exists an outer bi-Lipschitz extension \$f'\$ whose distortion is greater than that of \$f\$ by at most a constant factor. This result can be seen as a counterpart of the classic Kirszbraun theorem for outer bi-Lipschitz extensions. We also study outer bi-Lipschitz extensions of near-isometric maps and show upper and lower bounds for them. Then, we present applications of our results to prioritized and terminal dimension reduction problems. * We prove a prioritized variant of the Johnson-Lindenstrauss lemma: given a set of points \$X\textbackslash subset \textbackslash mathbb\{R\}\^d\$ of size \$N\$ and a permutation ("priority ranking") of \$X\$, there exists an embedding \$f\$ of \$X\$ into \$\textbackslash mathbb\{R\}\^\{O(\textbackslash log N)\}\$ with distortion \$O(\textbackslash log \textbackslash log N)\$ such that the point of rank \$j\$ has only \$O(\textbackslash log\^\{3 + \textbackslash varepsilon\} j)\$ non-zero coordinates - more specifically, all but the first \$O(\textbackslash log\^\{3+\textbackslash varepsilon\} j)\$ coordinates are equal to \$0\$; the distortion of \$f\$ restricted to the first \$j\$ points (according to the ranking) is at most \$O(\textbackslash log\textbackslash log j)\$. The result makes a progress towards answering an open question by Elkin, Filtser, and Neiman about prioritized dimension reductions. * We prove that given a set \$X\$ of \$N\$ points in \$\textbackslash mathbb\{R\}\^d\$, there exists a terminal dimension reduction embedding of \$\textbackslash mathbb\{R\}\^d\$ into \$\textbackslash mathbb\{R\}\^\{d'\}\$, where \$d' = O\textbackslash left(\textbackslash frac\{\textbackslash log N\}\{\textbackslash varepsilon\^4\}\textbackslash right)\$, which preserves distances \$\textbackslash |x-y\textbackslash |\$ between points \$x\textbackslash in X\$ and \$y \textbackslash in \textbackslash mathbb\{R\}\^\{d\}\$, up to a multiplicative factor of \$1 \textbackslash pm \textbackslash varepsilon\$. This improves a recent result by Elkin, Filtser, and Neiman. The dimension reductions that we obtain are nonlinear, and this nonlinearity is necessary.},
  pubstate = {preprint},
  keywords = {Computer Science - Computational Geometry,Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Mathematics - Metric Geometry},
  file = {/Users/matthewscott/Zotero/storage/GTAE5SNG/Mahabadi et al. - 2018 - Nonlinear Dimension Reduction via Outer Bi-Lipschi.pdf;/Users/matthewscott/Zotero/storage/SY2TPMKB/1811.html}
}

@book{matousekLecturesDiscreteGeometry2013,
  title = {Lectures on {{Discrete Geometry}}},
  author = {Matousek, Jiri},
  date = {2013-12-01},
  eprint = {K0fhBwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  abstract = {This book is primarily a textbook introduction to various areas of discrete geometry. In each area, it explains several key results and methods, in an accessible and concrete manner. It also contains more advanced material in separate sections and thus it can serve as a collection of surveys in several narrower subfields. The main topics include: basics on convex sets, convex polytopes, and hyperplane arrangements; combinatorial complexity of geometric configurations; intersection patterns and transversals of convex sets; geometric Ramsey-type results; polyhedral combinatorics and high-dimensional convexity; and lastly, embeddings of finite metric spaces into normed spaces.},
  isbn = {978-1-4613-0039-7},
  langid = {english},
  pagetotal = {491},
  keywords = {Mathematics / Geometry / Algebraic,Mathematics / Geometry / Analytic,Mathematics / Geometry / General},
  file = {/Users/matthewscott/Zotero/storage/VLXUQJB7/bafykbzacedhmushm3tx553e4wjzue67h2ly5kzusojwj3wqjzm2yvbjlbwjgg}
}

@inproceedings{maurerConcentrationInequalitiesSubGaussian2021,
  title = {Concentration Inequalities under Sub-{{Gaussian}} and Sub-Exponential Conditions},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Maurer, Andreas and Pontil, Massimiliano},
  date = {2021},
  volume = {34},
  pages = {7588--7597},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/3e33b970f21d2fc65096871ea0d2c6e4-Abstract.html},
  urldate = {2023-03-20},
  abstract = {We prove analogues of the popular bounded difference inequality (also called McDiarmid's inequality) for functions of independent random variables under sub-gaussian and sub-exponential conditions. Applied to vector-valued concentration and the method of Rademacher complexities these inequalities allow an easy extension of uniform convergence results for PCA and linear regression to the case potentially unbounded input- and output variables.},
  file = {/Users/matthewscott/Zotero/storage/92F9HH7R/Maurer_Pontil_2021_Concentration inequalities under sub-Gaussian and sub-exponential conditions.pdf}
}

@online{MIT854Spring,
  title = {{{MIT}} 6.854 {{Spring}} 2016 {{Lecture}} 22: {{Compressed Sensing}} - {{YouTube}}},
  shorttitle = {{{MIT}} 6.854 {{Spring}} 2016 {{Lecture}} 22},
  url = {https://www.youtube.com/},
  urldate = {2022-02-28},
  abstract = {Enjoy the videos and music you love, upload original content, and share it all with friends, family, and the world on YouTube.},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/PYJYPN6V/watch.html}
}

@article{murrayActivationFunctionDesign2022,
  title = {Activation Function Design for Deep Networks: Linearity and Effective Initialisation},
  shorttitle = {Activation Function Design for Deep Networks},
  author = {Murray, M. and Abrol, V. and Tanner, J.},
  date = {2022-07-01},
  journaltitle = {Applied and Computational Harmonic Analysis},
  shortjournal = {Applied and Computational Harmonic Analysis},
  series = {Special {{Issue}} on {{Harmonic Analysis}} and {{Machine Learning}}},
  volume = {59},
  pages = {117--154},
  issn = {1063-5203},
  doi = {10.1016/j.acha.2021.12.010},
  url = {https://www.sciencedirect.com/science/article/pii/S1063520321001111},
  urldate = {2022-10-18},
  abstract = {The activation function deployed in a deep neural network has great influence on the performance of the network at initialisation, which in turn has implications for training. In this paper we study how to avoid two problems at initialisation identified in prior works: rapid convergence of pairwise input correlations, and vanishing and exploding gradients. We prove that both these problems can be avoided by choosing an activation function possessing a sufficiently large linear region around the origin, relative to the bias variance σb2 of the network's random initialisation. We demonstrate empirically that using such activation functions leads to tangible benefits in practice, both in terms of test and training accuracy and in terms of training time. Furthermore, we observe that the shape of the nonlinear activation outside the linear region appears to have a relatively limited impact on training. Finally, our results also allow us to train networks in a new hyperparameter regime, with a much larger bias variance than has previously been possible.},
  langid = {english},
  keywords = {Activation function design,Deep learning,Initialisation,Random networks},
  file = {/Users/matthewscott/Zotero/storage/JSUE9PC3/Murray et al. - 2022 - Activation function design for deep networks line.pdf;/Users/matthewscott/Zotero/storage/MYUNG42X/S1063520321001111.html}
}

@online{naderiIndependentMeasurementsGeneral2021,
  title = {Beyond {{Independent Measurements}}: {{General Compressed Sensing}} with {{GNN Application}}},
  shorttitle = {Beyond {{Independent Measurements}}},
  author = {Naderi, Alireza and Plan, Yaniv},
  date = {2021-10-30},
  number = {arXiv:2111.00327},
  eprint = {arXiv:2111.00327},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2111.00327},
  url = {http://arxiv.org/abs/2111.00327},
  urldate = {2023-01-19},
  abstract = {We consider the problem of recovering a structured signal \$\textbackslash mathbf\{x\} \textbackslash in \textbackslash mathbb\{R\}\^\{n\}\$ from noisy linear observations \$\textbackslash mathbf\{y\} =\textbackslash mathbf\{M\} \textbackslash mathbf\{x\}+\textbackslash mathbf\{w\}\$. The measurement matrix is modeled as \$\textbackslash mathbf\{M\} = \textbackslash mathbf\{B\}\textbackslash mathbf\{A\}\$, where \$\textbackslash mathbf\{B\} \textbackslash in \textbackslash mathbb\{R\}\^\{l \textbackslash times m\}\$ is arbitrary and \$\textbackslash mathbf\{A\} \textbackslash in \textbackslash mathbb\{R\}\^\{m \textbackslash times n\}\$ has independent sub-gaussian rows. By varying \$\textbackslash mathbf\{B\}\$, and the sub-gaussian distribution of \$\textbackslash mathbf\{A\}\$, this gives a family of measurement matrices which may have heavy tails, dependent rows and columns, and singular values with a large dynamic range. When the structure is given as a possibly non-convex cone \$T \textbackslash subset \textbackslash mathbb\{R\}\^\{n\}\$, an approximate empirical risk minimizer is proven to be a robust estimator if the effective number of measurements is sufficient, even in the presence of a model mismatch. In classical compressed sensing with independent (sub-)gaussian measurements, one asks how many measurements are needed to recover \$\textbackslash mathbf\{x\}\$? In our setting, however, the effective number of measurements depends on the properties of \$\textbackslash mathbf\{B\}\$. We show that the effective rank of \$\textbackslash mathbf\{B\}\$ may be used as a surrogate for the number of measurements, and if this exceeds the squared Gaussian mean width of \$(T-T) \textbackslash cap \textbackslash mathbb\{S\}\^\{n-1\}\$, then accurate recovery is guaranteed. Furthermore, we examine the special case of generative priors in detail, that is when \$\textbackslash mathbf\{x\}\$ lies close to \$T = \textbackslash mathrm\{ran\}(G)\$ and \$G: \textbackslash mathbb\{R\}\^k \textbackslash rightarrow \textbackslash mathbb\{R\}\^n\$ is a Generative Neural Network (GNN) with ReLU activation functions. Our work relies on a recent result in random matrix theory by Jeong, Li, Plan, and Yilmaz arXiv:2001.10631. .},
  pubstate = {preprint},
  keywords = {Computer Science - Information Theory,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/LTV6PHTH/Naderi and Plan - 2021 - Beyond Independent Measurements General Compresse.pdf;/Users/matthewscott/Zotero/storage/JDBG9FZZ/2111.html}
}

@thesis{naderiSparsityfreeCompressedSensing2022,
  title = {A Sparsity-Free Compressed Sensing Theory with Applications in Generative Model Recovery},
  author = {Naderi, Alireza},
  date = {2022},
  institution = {{University of British Columbia}},
  doi = {10.14288/1.0412955},
  url = {https://open.library.ubc.ca/soa/cIRcle/collections/ubctheses/24/items/1.0412955},
  urldate = {2023-01-18},
  abstract = {We study the problem of reconstructing a high-dimensional signal x∈ℝⁿ from a low-dimensional noisy linear measurement y=Mx+e∈ℝˡ, assuming x admits a certain structure. We model the measurement matrix as M=BA, with arbitrary B∈ℝˡˣᵐ and sub-gaussian A∈ℝᵐˣⁿ; therefore allowing for a},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/WC8D4KLC/Naderi - 2022 - A sparsity-free compressed sensing theory with app.pdf}
}

@online{narayananOptimalTerminalDimensionality2018,
  title = {Optimal Terminal Dimensionality Reduction in {{Euclidean}} Space},
  author = {Narayanan, Shyam and Nelson, Jelani},
  date = {2018-10-22},
  number = {arXiv:1810.09250},
  eprint = {arXiv:1810.09250},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1810.09250},
  url = {http://arxiv.org/abs/1810.09250},
  urldate = {2022-12-07},
  abstract = {Let \$\textbackslash varepsilon\textbackslash in(0,1)\$ and \$X\textbackslash subset\textbackslash mathbb R\^d\$ be arbitrary with \$|X|\$ having size \$n{$>$}1\$. The Johnson-Lindenstrauss lemma states there exists \$f:X\textbackslash rightarrow\textbackslash mathbb R\^m\$ with \$m = O(\textbackslash varepsilon\^\{-2\}\textbackslash log n)\$ such that \$\$ \textbackslash forall x\textbackslash in X\textbackslash{} \textbackslash forall y\textbackslash in X, \textbackslash |x-y\textbackslash |\_2 \textbackslash le \textbackslash |f(x)-f(y)\textbackslash |\_2 \textbackslash le (1+\textbackslash varepsilon)\textbackslash |x-y\textbackslash |\_2 . \$\$ We show that a strictly stronger version of this statement holds, answering one of the main open questions of [MMMR18]: "\$\textbackslash forall y\textbackslash in X\$" in the above statement may be replaced with "\$\textbackslash forall y\textbackslash in\textbackslash mathbb R\^d\$", so that \$f\$ not only preserves distances within \$X\$, but also distances to \$X\$ from the rest of space. Previously this stronger version was only known with the worse bound \$m = O(\textbackslash varepsilon\^\{-4\}\textbackslash log n)\$. Our proof is via a tighter analysis of (a specific instantiation of) the embedding recipe of [MMMR18].},
  pubstate = {preprint},
  keywords = {Computer Science - Data Structures and Algorithms,Mathematics - Functional Analysis,Statistics - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/43RSM9G9/Narayanan and Nelson - 2018 - Optimal terminal dimensionality reduction in Eucli.pdf;/Users/matthewscott/Zotero/storage/RQVL2DM5/1810.html}
}

@book{nariciTopologicalVectorSpaces2010,
  title = {Topological {{Vector Spaces}}},
  author = {Narici, Lawrence and Beckenstein, Edward},
  date = {2010-07-22},
  edition = {2},
  publisher = {{Chapman and Hall/CRC}},
  location = {{New York}},
  doi = {10.1201/9781584888673},
  abstract = {With many new concrete examples and historical notes, Topological Vector Spaces, Second Edition provides one of the most thorough and up-to-date treatments of the Hahn-Banach theorem. This edition explores the theorem's connection with the axiom of choice, discusses the uniqueness of Hahn-Banach extensions, and includes an entirely new chapter on v},
  isbn = {978-0-429-14789-0},
  pagetotal = {628}
}

@article{nielsenCommonKnowledgeAggregate1990,
  title = {Common {{Knowledge}} of an {{Aggregate}} of {{Expectations}}},
  author = {Nielsen, Lars Tyge and Brandenburger, Adam and Geanakoplos, John and McKelvey, Richard and Page, Talbot},
  date = {1990},
  journaltitle = {Econometrica},
  volume = {58},
  number = {5},
  eprint = {2938308},
  eprinttype = {jstor},
  pages = {1235--1239},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {0012-9682},
  doi = {10.2307/2938308},
  url = {https://www.jstor.org/stable/2938308},
  urldate = {2022-03-01},
  file = {/Users/matthewscott/Zotero/storage/L37DHEFT/Nielsen et al. - 1990 - Common Knowledge of an Aggregate of Expectations.pdf}
}

@article{niyogiFindingHomologySubmanifolds,
  title = {Finding the {{Homology}} of {{Submanifolds}} with {{High Conﬁdence}} from {{Random Samples}}},
  author = {Niyogi, P and Smale, S and Weinberger, S},
  pages = {31},
  abstract = {Recently there has been a lot of interest in geometrically motivated approaches to data analysis in high dimensional spaces. We consider the case where data is drawn from sampling a probability distribution that has support on or near a submanifold of Euclidean space. We show how to “learn” the homology of the submanifold with high confidence. We discuss an algorithm to do this and provide learning-theoretic complexity bounds. Our bounds are obtained in terms of a condition number that limits the curvature and nearness to self-intersection of the submanifold. We are also able to treat the situation where the data is “noisy” and lies near rather than on the submanifold in question.},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/THXI7PM3/Niyogi et al. - Finding the Homology of Submanifolds with High Con.pdf}
}

@inproceedings{pappDropGNNRandomDropouts2021,
  title = {{{DropGNN}}: {{Random Dropouts Increase}} the {{Expressiveness}} of {{Graph Neural Networks}}},
  shorttitle = {{{DropGNN}}},
  author = {Papp, Pál András and Martinkus, Karolis and Faber, Lukas and Wattenhofer, Roger},
  date = {2021-05-21},
  url = {https://openreview.net/forum?id=fpQojkIV5q8},
  urldate = {2022-05-24},
  abstract = {We devise a new GNN variant (DropGNN) with larger expressive power in both theory and practice},
  eventtitle = {Advances in {{Neural Information Processing Systems}}},
  langid = {english},
  keywords = {\#Dropout,\#GraphNeuralNets},
  file = {/Users/matthewscott/Zotero/storage/C2L8MGSK/Papp et al. - 2021 - DropGNN Random Dropouts Increase the Expressivene.pdf;/Users/matthewscott/Zotero/storage/HAD24IV7/forum.html}
}

@article{papyanTheoreticalFoundationsDeep2018,
  title = {Theoretical {{Foundations}} of {{Deep Learning}} via {{Sparse Representations}}: {{A Multilayer Sparse Model}} and {{Its Connection}} to {{Convolutional Neural Networks}}},
  shorttitle = {Theoretical {{Foundations}} of {{Deep Learning}} via {{Sparse Representations}}},
  author = {Papyan, Vardan and Romano, Yaniv and Sulam, Jeremias and Elad, Michael},
  date = {2018-07},
  journaltitle = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {35},
  number = {4},
  pages = {72--89},
  issn = {1053-5888, 1558-0792},
  doi = {10.1109/MSP.2018.2820224},
  url = {https://ieeexplore.ieee.org/document/8398588/},
  urldate = {2022-02-27},
  langid = {english},
  keywords = {Dictionnary Learning,Neural Networks},
  file = {/Users/matthewscott/Zotero/storage/45V392PA/Papyan et al. - 2018 - Theoretical Foundations of Deep Learning via Spars.pdf}
}

@book{peterson12RulesLife2018,
  title = {12 {{Rules}} for {{Life}}: {{An Antidote}} to {{Chaos}}},
  shorttitle = {12 {{Rules}} for {{Life}}},
  author = {Peterson, Jordan B.},
  date = {2018-01-16},
  eprint = {u8w_DwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Penguin Books Limited}},
  abstract = {The \#1 Sunday Times and International Bestseller from 'the most influential public intellectual in the Western world right now' (New York Times)What are the most valuable things that everyone should know? Acclaimed clinical psychologist Jordan Peterson has influenced the modern understanding of personality, and now he has become one of the world's most popular public thinkers, with his lectures on topics from the Bible to romantic relationships to mythology drawing tens of millions of viewers. In an era of unprecedented change and polarizing politics, his frank and refreshing message about the value of individual responsibility and ancient wisdom has resonated around the world.In this book, he provides twelve profound and practical principles for how to live a meaningful life, from setting your house in order before criticising others to comparing yourself to who you were yesterday, not someone else today. Happiness is a pointless goal, he shows us. Instead we must search for meaning, not for its own sake, but as a defence against the suffering that is intrinsic to our existence. Drawing on vivid examples from the author's clinical practice and personal life, cutting edge psychology and philosophy, and lessons from humanity's oldest myths and stories, 12 Rules for Life offers a deeply rewarding antidote to the chaos in our lives: eternal truths applied to our modern problems.},
  isbn = {978-0-241-35165-9},
  langid = {english},
  pagetotal = {500},
  keywords = {Philosophy / General,Philosophy / Political,Philosophy / Social,Psychology / Clinical Psychology,Self-Help / Personal Growth / General,Self-Help / Personal Growth / Happiness,Self-Help / Personal Growth / Success,Self-Help / Self-Management / General}
}

@book{pinkDriveSurprisingTruth2011,
  title = {Drive: {{The Surprising Truth About What Motivates Us}}},
  shorttitle = {Drive},
  author = {Pink, Daniel H.},
  date = {2011-04-05},
  publisher = {{Penguin}},
  abstract = {The New York Times bestseller that gives readers a paradigm-shattering new way to think about motivation from the author of When: The Scientific Secrets of Perfect Timing Most people believe that the best way to motivate is with rewards like money—the carrot-and-stick approach. That's a mistake, says Daniel H. Pink (author of To Sell Is Human: The Surprising Truth About Motivating Others). In this provocative and persuasive new book, he asserts that the secret to high performance and satisfaction-at work, at school, and at home—is the deeply human need to direct our own lives, to learn and create new things, and to do better by ourselves and our world. Drawing on four decades of scientific research on human motivation, Pink exposes the mismatch between what science knows and what business does—and how that affects every aspect of life. He examines the three elements of true motivation—autonomy, mastery, and purpose-and offers smart and surprising techniques for putting these into action in a unique book that will change how we think and transform how we live.},
  isbn = {978-1-101-52438-1},
  langid = {english},
  pagetotal = {275},
  keywords = {Business & Economics / Leadership,Business & Economics / Motivational,Business & Economics / Sales & Selling / Management}
}

@article{polakPerformanceBoundsGrouped2015,
  title = {Performance {{Bounds}} for {{Grouped Incoherent Measurements}} in {{Compressive Sensing}}},
  author = {Polak, Adam C. and Duarte, Marco F. and Goeckel, Dennis L.},
  date = {2015-06},
  journaltitle = {IEEE Transactions on Signal Processing},
  shortjournal = {IEEE Trans. Signal Process.},
  volume = {63},
  number = {11},
  pages = {2877--2887},
  issn = {1053-587X, 1941-0476},
  doi = {10.1109/TSP.2015.2412912},
  url = {http://ieeexplore.ieee.org/document/7060659/},
  urldate = {2022-12-19},
  abstract = {Compressive sensing (CS) allows for acquisition of sparse signals at sampling rates significantly lower than the Nyquist rate required for bandlimited signals. Recovery guarantees for CS are generally derived based on the assumption that measurement projections are selected independently at random. However, for many practical signal acquisition applications, including medical imaging and remote sensing, this assumption is violated as the projections must be taken in groups. In this paper, we consider such applications and derive requirements on the number of measurements needed for successful recovery of signals when groups of dependent projections are taken at random. We find a penalty factor on the number of required measurements with respect to the standard CS scheme that employs conventional independent measurement selection and evaluate the accuracy of the predicted penalty through simulations.},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/Z6BFMDMM/Polak et al. - 2015 - Performance Bounds for Grouped Incoherent Measurem.pdf}
}

@article{puyVariableDensityCompressive2011,
  title = {On {{Variable Density Compressive Sampling}}},
  author = {Puy, Gilles and Vandergheynst, Pierre and Wiaux, Yves},
  date = {2011-10},
  journaltitle = {IEEE Signal Processing Letters},
  shortjournal = {IEEE Signal Process. Lett.},
  volume = {18},
  number = {10},
  eprint = {1109.6202},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  pages = {595--598},
  issn = {1070-9908, 1558-2361},
  doi = {10.1109/LSP.2011.2163712},
  url = {http://arxiv.org/abs/1109.6202},
  urldate = {2022-12-16},
  abstract = {We advocate an optimization procedure for variable density sampling in the context of compressed sensing. In this perspective, we introduce a minimization problem for the coherence between the sparsity and sensing bases, whose solution provides an optimized sampling profile. This minimization problem is solved with the use of convex optimization algorithms. We also propose a refinement of our technique when prior information is available on the signal support in the sparsity basis. The effectiveness of the method is confirmed by numerical experiments. Our results also provide a theoretical underpinning to state-of-the-art variable density Fourier sampling procedures used in magnetic resonance imaging.},
  keywords = {Computer Science - Information Theory},
  file = {/Users/matthewscott/Zotero/storage/JFR5WSWW/Puy et al. - 2011 - On Variable Density Compressive Sampling.pdf;/Users/matthewscott/Zotero/storage/GN6JHL2G/1109.html}
}

@article{rauhutCompressiveSensingStructured2010,
  title = {Compressive {{Sensing}} and {{Structured Random Matrices}}},
  author = {Rauhut, Holger},
  date = {2010},
  journaltitle = {De Gruyter},
  abstract = {These notes give a mathematical introduction to compressive sensing focusing on recovery using 1-minimization and structured random matrices. An emphasis is put on techniques for proving probabilistic estimates for condition numbers of structured random matrices. Estimates of this type are key to providing conditions that ensure exact or approximate recovery of sparse vectors using 1-minimization.},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/D3TQSMNZ/Rauhut - Compressive Sensing and Structured Random Matrices.pdf}
}

@online{rauhutSparseLegendreExpansions2011,
  title = {Sparse {{Legendre}} Expansions via \$\textbackslash ell\_1\$ Minimization},
  author = {Rauhut, Holger and Ward, Rachel},
  date = {2011-04-02},
  number = {arXiv:1003.0251},
  eprint = {arXiv:1003.0251},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1003.0251},
  urldate = {2022-12-16},
  abstract = {We consider the problem of recovering polynomials that are sparse with respect to the basis of Legendre polynomials from a small number of random samples. In particular, we show that a Legendre s-sparse polynomial of maximal degree N can be recovered from m = O(s log\^4(N)) random samples that are chosen independently according to the Chebyshev probability measure. As an efficient recovery method, l1-minimization can be used. We establish these results by verifying the restricted isometry property of a preconditioned random Legendre matrix. We then extend these results to a large class of orthogonal polynomial systems, including the Jacobi polynomials, of which the Legendre polynomials are a special case. Finally, we transpose these results into the setting of approximate recovery for functions in certain infinite-dimensional function spaces.},
  pubstate = {preprint},
  keywords = {41A10; 42C05; 94A20; 42A61; 60B20; 15A12; 65F35; 15B52; 94A12,Mathematics - Classical Analysis and ODEs,Mathematics - Functional Analysis,Mathematics - Numerical Analysis,Mathematics - Probability},
  file = {/Users/matthewscott/Zotero/storage/ZYU6MJ4B/Rauhut and Ward - 2011 - Sparse Legendre expansions via $ell_1$ minimizati.pdf;/Users/matthewscott/Zotero/storage/LPRH3ZC7/1003.html}
}

@book{riordanIntroductionCombinatorialAnalysis2002,
  title = {Introduction to {{Combinatorial Analysis}}},
  author = {Riordan, John},
  date = {2002-12-13},
  publisher = {{Dover Publications}},
  location = {{Mineola, N.Y}},
  isbn = {978-0-486-42536-8},
  langid = {english},
  pagetotal = {256}
}

@online{riouxMaximumEntropyMean2020,
  title = {The {{Maximum Entropy}} on the {{Mean Method}} for {{Image Deblurring}}},
  author = {Rioux, Gabriel and Choksi, Rustum and Hoheisel, Tim and Marechal, Pierre and Scarvelis, Christopher},
  date = {2020-10-20},
  number = {arXiv:2002.10434},
  eprint = {arXiv:2002.10434},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2002.10434},
  url = {http://arxiv.org/abs/2002.10434},
  urldate = {2022-10-13},
  abstract = {Image deblurring is a notoriously challenging ill-posed inverse problem. In recent years, a wide variety of approaches have been proposed based upon regularization at the level of the image or on techniques from machine learning. We propose an alternative approach, shifting the paradigm towards regularization at the level of the probability distribution on the space of images. Our method is based upon the idea of maximum entropy on the mean wherein we work at the level of the probability density function of the image whose expectation is our estimate of the ground truth. Using techniques from convex analysis and probability theory, we show that the method is computationally feasible and amenable to very large blurs. Moreover, when images are imbedded with symbology (a known pattern), we show how our method can be applied to approximate the unknown blur kernel with remarkable effects. While our method is stable with respect to small amounts of noise, it does not actively denoise. However, for moderate to large amounts of noise, it performs well by preconditioned denoising with a state of the art method.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{rudelsonRandomVectorsIsotropic1999,
  title = {Random {{Vectors}} in the {{Isotropic Position}}},
  author = {Rudelson, M.},
  date = {1999-05-10},
  journaltitle = {Journal of Functional Analysis},
  shortjournal = {Journal of Functional Analysis},
  volume = {164},
  number = {1},
  pages = {60--72},
  issn = {0022-1236},
  doi = {10.1006/jfan.1998.3384},
  url = {https://www.sciencedirect.com/science/article/pii/S0022123698933845},
  urldate = {2022-08-25},
  abstract = {Letybe a random vector in Rn, satisfyingEy⊗y=id.LetMbe a natural number and lety1,~…,~yMbe independent copies ofy. We study the question of approximation of the identity operator by finite sums of the tensorsyi⊗yi. We prove that for some absolute constantCE1M∑i=1Myi⊗yi−id⩽C·lognM·(E‖y‖logM)1/logM,provided that the last expression is smaller than 1. We apply this estimate to improve a result of Bourgain concerning the number of random points needed to bring a convex body into a nearly isotropic position.},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/QRRESUVM/Rudelson_isotropic_vectors.pdf.pdf}
}

@article{rudelsonSparseReconstructionFourier2008,
  title = {On Sparse Reconstruction from {{Fourier}} and {{Gaussian}} Measurements},
  author = {Rudelson, Mark and Vershynin, Roman},
  date = {2008-08},
  journaltitle = {Communications on Pure and Applied Mathematics},
  shortjournal = {Comm. Pure Appl. Math.},
  volume = {61},
  number = {8},
  pages = {1025--1045},
  issn = {00103640, 10970312},
  doi = {10.1002/cpa.20227},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/cpa.20227},
  urldate = {2022-02-27},
  abstract = {This paper improves upon best-known guarantees for exact reconstruction of a sparse signal f from a small universal sample of Fourier measurements. The method for reconstruction that has recently gained momentum in the sparse approximation theory is to relax this highly nonconvex problem to a convex problem and then solve it as a linear program. We show that there exists a set of frequencies such that one can exactly reconstruct every r-sparse signal f of length n from its frequencies in , using the convex relaxation, and has size k.r; n/ D O.r log.n/ log2.r/ log.r log n// D O.r log4 n/: A random set satisfies this with high probability. This estimate is optimal within the log log n and log3 r factors. We also give a relatively short argument for a similar problem with k.r; n/ rŒ12 C 8 log.n=r/ Gaussian measurements. We use methods of geometric functional analysis and probability theory in Banach spaces, which makes our arguments quite short.},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/BDQG6CHR/Rudelson and Vershynin - 2008 - On sparse reconstruction from Fourier and Gaussian.pdf}
}

@misc{ScalingTailsLipshitz,
  title = {Scaling of {{Tails Lipshitz Formulation}}},
  file = {/Users/matthewscott/Zotero/storage/9XSEEY34/_.pdf}
}

@article{scarlettTheoreticalPerspectivesDeep2022,
  title = {Theoretical {{Perspectives}} on {{Deep Learning Methods}} in {{Inverse Problems}}},
  author = {Scarlett, Jonathan and Heckel, Reinhard and Rodrigues, Miguel R. D. and Hand, Paul and Eldar, Yonina C.},
  date = {2022-01-29},
  journaltitle = {IEEE JSAIT (Special Issue on Deep Learning for Inverse Problems)},
  eprint = {2206.14373},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, math, stat},
  doi = {10.48550/arXiv.2206.14373},
  url = {http://arxiv.org/abs/2206.14373},
  urldate = {2023-03-10},
  abstract = {In recent years, there have been significant advances in the use of deep learning methods in inverse problems such as denoising, compressive sensing, inpainting, and super-resolution. While this line of works has predominantly been driven by practical algorithms and experiments, it has also given rise to a variety of intriguing theoretical problems. In this paper, we survey some of the prominent theoretical developments in this line of works, focusing in particular on generative priors, untrained neural network priors, and unfolding algorithms. In addition to summarizing existing results in these topics, we highlight several ongoing challenges and open problems.},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/VN3XGTHX/Scarlett et al_2023_Theoretical Perspectives on Deep Learning Methods in Inverse Problems.pdf;/Users/matthewscott/Zotero/storage/85M2TI9S/2206.html}
}

@unpublished{scottConcentrationReluSubGaussian2022,
  title = {Concentration of {{Relu}} of {{Sub-Gaussian Matrices}} on {{Sets}}},
  author = {Scott, Matthew},
  date = {2022},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/U6NC4VYG/Scott - Final Project CPSC536 Concentration of Relu of Su.pdf}
}

@misc{scottCPSC536Final,
  title = {{{CPSC}} 536 {{Final}} Project {{Proposal}}},
  author = {Scott, Matthew},
  file = {/Users/matthewscott/Zotero/storage/H6D9EBYU/Scott - CPSC 536 Final project Proposal.pdf}
}

@misc{scottDataAugmentationFormalisation,
  title = {Data {{Augmentation Formalisation}}},
  author = {Scott, Matthew},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/G25D9MCE/Scott - Data Augmentation Formalisation.pdf;/Users/matthewscott/Zotero/storage/R4JYNF7F/Scott - Data Augmentation Formalisation.pdf}
}

@misc{scottFDRDerivativeMarkov2020,
  title = {{{FDR}} as a {{Derivative}} on {{Markov Chains}}},
  author = {Scott, Matthew},
  date = {2020},
  file = {/Users/matthewscott/Zotero/storage/WWNVXXTF/Scott - FDR as a Derivative on Markov Chains.pdf}
}

@misc{scottKLDivergenceExploration,
  title = {{{KL}}\_{{Divergence Exploration}}},
  author = {Scott, Matthew},
  file = {/Users/matthewscott/Zotero/storage/TYUDDFY3/Scott - KL_Divergence Exploration.pdf}
}

@misc{scottLossFramework,
  title = {Loss {{Framework}}},
  author = {Scott, Matthew},
  file = {/Users/matthewscott/Zotero/storage/8W5V7IYK/Scott - Loss Framework.pdf}
}

@misc{scottLossTermsOverview,
  title = {Loss {{Terms Overview}}},
  author = {Scott, Matthew},
  file = {/Users/matthewscott/Zotero/storage/8F32ID5K/Scott - Loss Terms Overview.pdf}
}

@article{scottMath396FinalProject,
  title = {Math396 {{Final Project}}: {{The Kullback-Leibler Divergence}} in {{Neural Net Training}}},
  author = {Scott, Matthew},
  pages = {9},
  abstract = {In this final project, we explore the Kullback-Leibler divergence in the context of losses for neural networks. We first attempt to understand it from an information-theoretic standpoint. Then we discuss the relationship between the KL divergence and maximum likelihood estimation, which we look at through bayesian model selection. We then discuss invariances of the KL divergence under transformations of the probability distributions and transformations in the logit space. We then look at how the KL divergence can be estimated empirically from i.i.d. samples in the density estimation problem, which leads us to frame the classification problem as parallel density estimation problems, which leads to pathways for finding theoretical bounds. We conclude with a pseudo-proof under reasonable assumptions that the KL divergence loss is the only possible loss for the training of neural nets.},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/RFBBX4YJ/Scott - Math396 Final Project The Kullback-Leibler Diverg.pdf}
}

@misc{scottReviewFDRCOMP599,
  title = {Review of {{FDR COMP599 Final Project}}},
  author = {Scott, Matthew},
  file = {/Users/matthewscott/Zotero/storage/IDC5K4BI/Scott - Review of FDR COMP599 Final Project.pdf}
}

@unpublished{scottRIPSubsampledFourier2022,
  title = {{{RIP}} from {{Subsampled Fourier}} on the {{Almost Sparse Prior}}.},
  author = {Scott, Matthew},
  date = {2022},
  file = {/Users/matthewscott/Zotero/storage/JIRQ3G8F/Scott - RIP from Subsampled Fourier on the Almost Sparse P.pdf}
}

@misc{scottSubspaceFarBasis,
  title = {Subspace Far {{From Basis}} and {{Matrix Bernstein}}},
  author = {Scott, Matthew},
  file = {/Users/matthewscott/Zotero/storage/SSS6Q9A9/Scott - Subspace far From Basis.pdf}
}

@misc{scottSymmetryConservedQuantities,
  title = {Symmetry and {{Conserved Quantities}}},
  author = {Scott, Matthew},
  file = {/Users/matthewscott/Zotero/storage/MBWTS99M/Scott - Symmetry and Conserved Quantities.pdf}
}

@misc{scottThermoStatsMech,
  title = {Thermo and {{Stats Mech Crash Course}}},
  author = {Scott, Matthew},
  file = {/Users/matthewscott/Zotero/storage/389MBPKN/Scott - Thermo and Stats Mech Crash Course.pdf}
}

@inproceedings{shamshadDeepPtychSubsampled2019,
  title = {Deep {{Ptych}}: {{Subsampled Fourier Ptychography Using Generative Priors}}},
  shorttitle = {Deep {{Ptych}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Shamshad, Fahad and Abbas, Farwa and Ahmed, Ali},
  date = {2019-05},
  pages = {7720--7724},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2019.8682179},
  abstract = {This paper proposes a novel framework to regularize the highly ill-posed and non-linear Fourier ptychography problem using generative models. We demonstrate experimentally that our proposed algorithm, Deep Ptych, outperforms the existing Fourier ptychography techniques, in terms of quality of reconstruction and robustness against noise, using far fewer samples. We further modify the proposed approach to allow the generative model to explore solutions outside the range, leading to improved performance.},
  eventtitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  keywords = {Cameras,Computational modeling,Fourier ptychography,generative models,Generators,Image reconstruction,Image resolution,noise robustness,phase retrieval,subsampling,Training},
  file = {/Users/matthewscott/Zotero/storage/JB2UKQKN/8682179.html}
}

@online{shwartz-zivOpeningBlackBox2017,
  title = {Opening the {{Black Box}} of {{Deep Neural Networks}} via {{Information}}},
  author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
  date = {2017-04-29},
  number = {arXiv:1703.00810},
  eprint = {arXiv:1703.00810},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1703.00810},
  urldate = {2022-08-14},
  abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the \textbackslash textit\{Information Plane\}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on \{\textbackslash emph compression\} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/SRSZB73F/1703.00810.pdf}
}

@book{stanleyEnumerativeCombinatoricsVolume2011,
  title = {Enumerative {{Combinatorics}}: {{Volume}} 1},
  shorttitle = {Enumerative {{Combinatorics}}},
  author = {Stanley, Richard P.},
  date = {2011-12-12},
  edition = {2 edition},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge, NY}},
  isbn = {978-1-107-60262-5},
  langid = {english},
  pagetotal = {642},
  file = {/Users/matthewscott/Zotero/storage/9XJVERSF/enu_comb_stanley.pdf}
}

@article{stevicNoteBinomialPartial2015,
  title = {Note on the Binomial Partial Difference Equation},
  author = {Stevic, Stevo},
  date = {2015},
  journaltitle = {Electronic Journal of Qualitative Theory of Differential Equations},
  volume = {2015},
  number = {96},
  pages = {1--11},
  issn = {1417-3875},
  url = {http://www.math.u-szeged.hu/ejqtde/p4527.pdf},
  urldate = {2022-09-01},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/99IBU7ER/a3b7b05d2c780306df1b25249981c856518c.pdf}
}

@misc{SummerFDRBaselines,
  title = {Summer {{FDR}}, {{Baselines}} and {{RL Setting}}},
  file = {/Users/matthewscott/Zotero/storage/UF63SD6L/Summer FDR, Baselines and RL Setting.pdf}
}

@online{tannerC6TheoriesDeep,
  title = {C6.5 {{Theories}} of {{Deep Learning}} (2020-2021) | {{Mathematical Institute Course Management}}},
  author = {Tanner, Jared},
  url = {https://courses-archive.maths.ox.ac.uk/node/49612/materials},
  urldate = {2022-11-11}
}

@inproceedings{tishbyDeepLearningInformation2015,
  title = {Deep Learning and the Information Bottleneck Principle},
  booktitle = {2015 {{IEEE Information Theory Workshop}} ({{ITW}})},
  author = {Tishby, Naftali and Zaslavsky, Noga},
  date = {2015-04},
  pages = {1--5},
  doi = {10.1109/ITW.2015.7133169},
  abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
  eventtitle = {2015 {{IEEE Information Theory Workshop}} ({{ITW}})},
  keywords = {Bifurcation,Complexity theory,Computer architecture,Distortion,Feature extraction,Mutual information,Training},
  file = {/Users/matthewscott/Zotero/storage/SXA2YUF2/Tishby and Zaslavsky - 2015 - Deep learning and the information bottleneck princ.pdf;/Users/matthewscott/Zotero/storage/SZY8A6CM/7133169.html}
}

@article{tongUniversityCambridgePart,
  title = {University of {{Cambridge Part III Mathematical Tripos}}},
  author = {Tong, Dr David},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/TLMCADTM/Tong - University of Cambridge Part III Mathematical Trip.pdf}
}

@article{troppUserFriendlyTailBounds2012,
  title = {User-{{Friendly Tail Bounds}} for {{Sums}} of {{Random Matrices}}},
  author = {Tropp, Joel A.},
  date = {2012-08},
  journaltitle = {Foundations of Computational Mathematics},
  shortjournal = {Found Comput Math},
  volume = {12},
  number = {4},
  pages = {389--434},
  issn = {1615-3375, 1615-3383},
  doi = {10.1007/s10208-011-9099-z},
  url = {http://link.springer.com/10.1007/s10208-011-9099-z},
  urldate = {2022-07-13},
  abstract = {This paper presents new probability inequalities for sums of independent, random, self-adjoint matrices. These results place simple and easily verifiable hypotheses on the summands, and they deliver strong conclusions about the largedeviation behavior of the maximum eigenvalue of the sum. Tail bounds for the norm of a sum of random rectangular matrices follow as an immediate corollary. The proof techniques also yield some information about matrix-valued martingales.},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/JTM9XTEC/Tropp - 2012 - User-Friendly Tail Bounds for Sums of Random Matri.pdf}
}

@inproceedings{turnerOptimalPoliciesTend2021,
  title = {Optimal {{Policies Tend To Seek Power}}},
  author = {Turner, Alexander Matt and Smith, Logan Riggs and Shah, Rohin and Critch, Andrew and Tadepalli, Prasad},
  date = {2021-05-21},
  url = {https://openreview.net/forum?id=l7-DBWawSZH},
  urldate = {2022-05-24},
  abstract = {Power-seeking incentives arise from certain symmetries in the agent's environment.},
  eventtitle = {Advances in {{Neural Information Processing Systems}}},
  langid = {english},
  keywords = {\#RL},
  file = {/Users/matthewscott/Zotero/storage/SPCB9SYS/Turner et al. - 2021 - Optimal Policies Tend To Seek Power.pdf;/Users/matthewscott/Zotero/storage/BP5WZ2PV/forum.html}
}

@book{vershyninHighDimensionalProbabilityIntroduction2018,
  title = {High-{{Dimensional Probability}}: {{An Introduction}} with {{Applications}} in {{Data Science}}},
  shorttitle = {High-{{Dimensional Probability}}},
  author = {Vershynin, Roman},
  date = {2018-09-27},
  eprint = {TahxDwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Cambridge University Press}},
  abstract = {High-dimensional probability offers insight into the behavior of random vectors, random matrices, random subspaces, and objects used to quantify uncertainty in high dimensions. Drawing on ideas from probability, analysis, and geometry, it lends itself to applications in mathematics, statistics, theoretical computer science, signal processing, optimization, and more. It is the first to integrate theory, key tools, and modern applications of high-dimensional probability. Concentration inequalities form the core, and it covers both classical results such as Hoeffding's and Chernoff's inequalities and modern developments such as the matrix Bernstein's inequality. It then introduces the powerful methods based on stochastic processes, including such tools as Slepian's, Sudakov's, and Dudley's inequalities, as well as generic chaining and bounds based on VC dimension. A broad range of illustrations is embedded throughout, including classical and modern results for covariance estimation, clustering, networks, semidefinite programming, coding, dimension reduction, matrix completion, machine learning, compressed sensing, and sparse regression.},
  isbn = {978-1-108-24454-1},
  langid = {english},
  pagetotal = {299},
  keywords = {Business & Economics / Econometrics,Computers / Optical Data Processing,Language Arts & Disciplines / Library & Information Science / General,Mathematics / Probability & Statistics / General,Technology & Engineering / Signals & Signal Processing},
  file = {/Users/matthewscott/Zotero/storage/EJFK8JUU/Vershynin - 2018 - High-Dimensional Probability An Introduction with.pdf}
}

@online{vershyninMemoryCapacityNeural2020,
  title = {Memory Capacity of Neural Networks with Threshold and {{ReLU}} Activations},
  author = {Vershynin, Roman},
  date = {2020-06-02},
  number = {arXiv:2001.06938},
  eprint = {arXiv:2001.06938},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2001.06938},
  url = {http://arxiv.org/abs/2001.06938},
  urldate = {2022-08-13},
  abstract = {Overwhelming theoretical and empirical evidence shows that mildly overparametrized neural networks -- those with more connections than the size of the training data -- are often able to memorize the training data with \$100\textbackslash\%\$ accuracy. This was rigorously proved for networks with sigmoid activation functions and, very recently, for ReLU activations. Addressing a 1988 open question of Baum, we prove that this phenomenon holds for general multilayered perceptrons, i.e. neural networks with threshold activation functions, or with any mix of threshold and ReLU activations. Our construction is probabilistic and exploits sparsity.},
  pubstate = {preprint},
  keywords = {68Q32; 92B20,Computer Science - Information Theory,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/DUR4BQEK/Vershynin - 2020 - Memory capacity of neural networks with threshold .pdf;/Users/matthewscott/Zotero/storage/PN37TABT/2001.html}
}

@book{wainwrightHighDimensionalStatisticsNonAsymptotic2019,
  title = {High-{{Dimensional Statistics}}: {{A Non-Asymptotic Viewpoint}}},
  shorttitle = {High-{{Dimensional Statistics}}},
  author = {Wainwright, Martin J.},
  date = {2019-02-21},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge ; New York, NY}},
  abstract = {Recent years have witnessed an explosion in the volume and variety of data collected in all scientific disciplines and industrial settings. Such massive data sets present a number of challenges to researchers in statistics and machine learning. This book provides a self-contained introduction to the area of high-dimensional statistics, aimed at the first-year graduate level. It includes chapters that are focused on core methodology and theory - including tail bounds, concentration inequalities, uniform laws and empirical process, and random matrices - as well as chapters devoted to in-depth exploration of particular model classes - including sparse linear models, matrix models with rank constraints, graphical models, and various types of non-parametric models. With hundreds of worked examples and exercises, this text is intended both for courses and for self-study by graduate students and researchers in statistics, machine learning, and related fields who must understand, apply, and adapt modern statistical methods suited to large-scale data.},
  isbn = {978-1-108-49802-9},
  langid = {english},
  pagetotal = {568},
  file = {/Users/matthewscott/Zotero/storage/WQNVDKUV/Martin J. Wainwright - High-Dimensional Statistics A Non-Asymptotic Viewpoint-Cambridge University Press ( 2019).pdf}
}

@book{walkerSurveysCombinatorics19931993,
  title = {Surveys in Combinatorics, 1993},
  editor = {Walker, Keith},
  date = {1993-07-29},
  series = {London {{Mathematical Society}} Lecture Note Series},
  number = {187},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  eventtitle = {British {{Combinatorial Conference}}},
  isbn = {978-0-521-44857-4},
  langid = {english},
  pagetotal = {287},
  file = {/Users/matthewscott/Zotero/storage/8PH23ZSX/Walker - 1993 - Surveys in combinatorics, 1993.pdf}
}

@inproceedings{wangSelfSupervisedLearningDisentangled2021,
  title = {Self-{{Supervised Learning Disentangled Group Representation}} as {{Feature}}},
  author = {Wang, Tan and Yue, Zhongqi and Huang, Jianqiang and Sun, Qianru and Zhang, Hanwang},
  date = {2021-05-21},
  url = {https://openreview.net/forum?id=RQfcckT1M_4},
  urldate = {2022-05-24},
  abstract = {An iterative IRM algorithm for unsupervised feature disentanglement and self-supervised feature learning},
  eventtitle = {Advances in {{Neural Information Processing Systems}}},
  langid = {english},
  keywords = {\#Contrastive,\#GroupsNeuralNets},
  file = {/Users/matthewscott/Zotero/storage/6MZ62RMB/Wang et al. - 2021 - Self-Supervised Learning Disentangled Group Repres.pdf;/Users/matthewscott/Zotero/storage/7LEGCLIF/forum.html}
}

@article{weinsteinGeometricUnityAuthor,
  title = {Geometric {{Unity}}: {{Author}}’s {{Working Draft}}, v 1.0},
  author = {Weinstein, Eric},
  abstract = {An attempt is made to address a stylized question posed to Ernst Strauss by Albert Einstein regarding the amount of freedom present in the construction of our field theoretic universe.},
  langid = {english},
  file = {/Users/matthewscott/Zotero/storage/Y5M3AE6S/Weinstein - Geometric Unity Author’s Working Draft, v 1.0.pdf}
}

@book{wilfGeneratingfunctionology2005,
  title = {Generatingfunctionology},
  author = {Wilf, Herbert S.},
  date = {2005-12-20},
  edition = {3rd edition},
  publisher = {{AK Peters/CRC}},
  location = {{Wellesley, Mass}},
  isbn = {978-1-56881-279-3},
  langid = {english},
  pagetotal = {256},
  file = {/Users/matthewscott/Zotero/storage/YWMMX4T2/gfology2.pdf}
}

@inproceedings{wuDeepCompressedSensing2019,
  title = {Deep {{Compressed Sensing}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Wu, Yan and Rosca, Mihaela and Lillicrap, Timothy},
  date = {2019-05-24},
  pages = {6850--6860},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/wu19d.html},
  urldate = {2022-11-12},
  abstract = {Compressed sensing (CS) provides an elegant framework for recovering sparse signals from compressed measurements. For example, CS can exploit the structure of natural images and recover an image from only a few random measurements. Unlike popular autoencoding models, reconstruction in CS is posed as an optimisation problem that is separate from sensing. CS is flexible and data efficient, but its application has been restricted by the strong assumption of sparsity and costly reconstruction process. A recent approach that combines CS with neural network generators has removed the constraint of sparsity, but reconstruction remains slow. Here we propose a novel framework that significantly improves both the performance and speed of signal recovery by jointly training a generator and the optimisation process for reconstruction via meta-learning. We explore training the measurements with different objectives, and derive a family of models based on minimising measurement errors. We show that Generative Adversarial Nets (GANs) can be viewed as a special case in this family of models. Borrowing insights from the CS perspective, we develop a novel way of improving GANs using gradient information from the discriminator.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {Intermediate Layer},
  file = {/Users/matthewscott/Zotero/storage/J235IPDU/Wu et al. - 2019 - Deep Compressed Sensing.pdf;/Users/matthewscott/Zotero/storage/MSC4E8QS/Wu et al. - 2019 - Deep Compressed Sensing.pdf}
}

@online{wuGroupNormalization2018,
  title = {Group {{Normalization}}},
  author = {Wu, Yuxin and He, Kaiming},
  date = {2018-06-11},
  number = {arXiv:1803.08494},
  eprint = {arXiv:1803.08494},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1803.08494},
  urldate = {2022-11-27},
  abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6\% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/FCWJJU5V/Wu and He - 2018 - Group Normalization.pdf;/Users/matthewscott/Zotero/storage/KD6KU6RI/1803.html}
}

@inproceedings{yaidaFluctuationdissipationRelationsStochastic2018,
  title = {Fluctuation-Dissipation Relations for Stochastic Gradient Descent},
  author = {Yaida, Sho},
  date = {2018-12-21},
  eprint = {1810.00004},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1810.00004},
  url = {http://arxiv.org/abs/1810.00004},
  urldate = {2022-11-13},
  abstract = {The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  keywords = {Computer Science - Machine Learning,FDR,Statistics - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/M4N6HNGQ/Yaida - 2018 - Fluctuation-dissipation relations for stochastic g.pdf;/Users/matthewscott/Zotero/storage/II9HXJ3L/1810.html}
}

@online{yangDiffusionModelsComprehensive2022,
  title = {Diffusion {{Models}}: {{A Comprehensive Survey}} of {{Methods}} and {{Applications}}},
  shorttitle = {Diffusion {{Models}}},
  author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Shao, Yingxia and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  date = {2022-10-23},
  number = {arXiv:2209.00796},
  eprint = {arXiv:2209.00796},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/2209.00796},
  urldate = {2022-11-15},
  abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/matthewscott/Zotero/storage/NM6G2EBP/Yang et al. - 2022 - Diffusion Models A Comprehensive Survey of Method.pdf;/Users/matthewscott/Zotero/storage/VQBE475M/2209.html}
}
